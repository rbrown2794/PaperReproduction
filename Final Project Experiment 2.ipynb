{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b19916df",
   "metadata": {},
   "source": [
    "# CS 598 DLH Final Project Experiment 2\n",
    "by Regan Brown (rnbrown3, group 51)\n",
    "\n",
    "This is NOT intended as a bonus \"Descriptive notebook\". This contains all the source code for Experiment 2 of my final project.\n",
    "\n",
    "Please see the Experiment 1 notebook for most instructions and explanations. Assume steps and reasonings are the same unless otherwise specified.\n",
    "\n",
    "In order to ensure the proper environment to run this code, please download the latest versions of all libraries/imports mentioned in the code blocks. In addition, you will need to install necessary CoreNLP packages and start up an instance of the Stanford Core NLP server, following the instructions at https://github.com/nltk/nltk/wiki/Stanford-CoreNLP-API-in-NLTK\n",
    "\n",
    "You will need to either download the STS benchmark dataset from https://ixa2.si.ehu.eus/stswiki/index.php/STSbenchmark, or use the copies I have provided in my repo. You will need to replace the filepaths in the .read_csv statements below accordingly; I have commented these spots for extra clarity.\n",
    "\n",
    "## Preprocessing\n",
    "The first steps to take are loading in the STS benchmark dataset, then extracting the sentence pairs and scores/labels for each of the four subsets of data we are performing the experiment on. Then we initialize the Stanford parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55b5f221",
   "metadata": {},
   "outputs": [],
   "source": [
    "#be sure to start up Stanford Parser server following steps here: https://github.com/nltk/nltk/wiki/Stanford-CoreNLP-API-in-NLTK\n",
    "from nltk.parse import CoreNLPParser\n",
    "from nltk.tree import Tree, ParentedTree\n",
    "import time #for tracking time to run\n",
    "import tracemalloc #for tracking memory usage\n",
    "_START_RUNTIME = time.time()\n",
    "tracemalloc.start()\n",
    "\n",
    "#first, load in the STS training data\n",
    "import pandas as pd\n",
    "#REPLACE FILEPATH HERE: PATH TO TRAIN DATA\n",
    "df = pd.read_csv(r'C:\\Users\\rbrow\\Downloads\\Stsbenchmark\\Stsbenchmark\\stsbenchmark\\sts-train.csv', sep='\\t', on_bad_lines='skip', header=None, names=[\"genre\", \"file\", \"year\", 'idx', 'score', 'sentence1', 'sentence2'])\n",
    "MSRvid = df[df['file'] == 'MSRvid'] #1000 rows, all from 2012\n",
    "headlines = df[(df['file'] == 'headlines') & df['year'].str.startswith('2013')] #597 rows\n",
    "images2014 = df[(df['file'] == 'images') & df['year'].str.startswith('2014')] #497 rows\n",
    "images2015 = df[(df['file'] == 'images') & df['year'].str.startswith('2015')] #503 rows\n",
    "\n",
    "#load in the STS test data\n",
    "import csv\n",
    "#REPLACE FILEPATH HERE: PATH TO TEST DATA\n",
    "df = pd.read_csv(r'C:\\Users\\rbrow\\Downloads\\Stsbenchmark\\Stsbenchmark\\stsbenchmark\\sts-test.csv', quoting=csv.QUOTE_NONE, on_bad_lines='skip', sep='\\t', header=None, names=[\"genre\", \"file\", \"year\", 'idx', 'score', 'sentence1', 'sentence2'])\n",
    "MSRvid_test = df[df['file'] == 'MSRvid'] #250 rows, all from 2012\n",
    "headlines_test = df[(df['file'] == 'headlines') & df['year'].str.startswith('2013')] #72 rows\n",
    "images2014_test = df[(df['file'] == 'images') & df['year'].str.startswith('2014')] #135 rows\n",
    "images2015_test = df[(df['file'] == 'images') & df['year'].str.startswith('2015')] #115 rows\n",
    "\n",
    "#MSRvid variables\n",
    "labels = MSRvid['score'].tolist()\n",
    "sentence1 = MSRvid['sentence1'].tolist()\n",
    "sentence2 = MSRvid['sentence2'].tolist()\n",
    "labels_test = MSRvid_test['score'].tolist()\n",
    "sentence1_test = MSRvid_test['sentence1'].tolist()\n",
    "sentence2_test = MSRvid_test['sentence2'].tolist()\n",
    "\n",
    "#headlines variables\n",
    "labels_h = headlines['score'].tolist()\n",
    "sentence1_h = headlines['sentence1'].tolist()\n",
    "sentence2_h = headlines['sentence2'].tolist()\n",
    "labels_test_h = headlines_test['score'].tolist()\n",
    "sentence1_test_h = headlines_test['sentence1'].tolist()\n",
    "sentence2_test_h = headlines_test['sentence2'].tolist()\n",
    "\n",
    "#images2014 variables\n",
    "labels_4 = images2014['score'].tolist()\n",
    "sentence1_4 = images2014['sentence1'].tolist()\n",
    "sentence2_4 = images2014['sentence2'].tolist()\n",
    "labels_test_4 = images2014_test['score'].tolist()\n",
    "sentence1_test_4 = images2014_test['sentence1'].tolist()\n",
    "sentence2_test_4 = images2014_test['sentence2'].tolist()\n",
    "\n",
    "#images2015 variables\n",
    "labels_5 = images2015['score'].tolist()\n",
    "sentence1_5 = images2015['sentence1'].tolist()\n",
    "sentence2_5 = images2015['sentence2'].tolist()\n",
    "labels_test_5 = images2015_test['score'].tolist()\n",
    "sentence1_test_5 = images2015_test['sentence1'].tolist()\n",
    "sentence2_test_5 = images2015_test['sentence2'].tolist()\n",
    "\n",
    "#initialize the Stanford parser\n",
    "parser = CoreNLPParser(url='http://localhost:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b565f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementation of SPO algorithm as outlined in the paper's pseudocode (Algorithm 1)\n",
    "#to help make sense of this code, please check the label definitions here: https://stackoverflow.com/questions/1833252/java-stanford-nlp-part-of-speech-labels\n",
    "def spo(sentence):\n",
    "    tree = parser.raw_parse(sentence)\n",
    "    tree = next(tree) #need to pull the Tree out of the iter\n",
    "    \n",
    "    subject = \"\"\n",
    "    predicate = \"\"\n",
    "    obj = \"\"\n",
    "    for t in tree[0]:\n",
    "        if t.label() == 'NP': #identify subject\n",
    "            for s in t.subtrees():\n",
    "                for n in s.subtrees():\n",
    "                    if n.label().startswith(\"NN\"):\n",
    "                        subject = n[0]\n",
    "        if t.label() == 'VP': #identify predicate\n",
    "            for p in t.subtrees():\n",
    "                for m in p.subtrees():\n",
    "                    if m.label().startswith(\"VB\"):\n",
    "                        predicate = m[0]\n",
    "        if t.label() == 'VP': #identify object (code based on code found here: https://github.com/HassanElmadany/Extract-SVO/blob/master/Subject_Verb_Object_Extractor.py)\n",
    "            for k in t.subtrees(lambda n: n.label() in ['NP', 'PP', 'ADJP']):\n",
    "                if k.label() in ['NP', 'PP']:\n",
    "                    for c in k.subtrees(lambda c: c.label().startswith('NN')):\n",
    "                        obj = c[0]\n",
    "                else:\n",
    "                    for c in k.subtrees(lambda c: c.label().startswith('JJ')):\n",
    "                        obj = c[0]\n",
    "    return [subject, predicate, obj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bc81af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse first sentences in sentence pairs, for both train and test sets\n",
    "#for MSRvid\n",
    "sentence1_parsed = []\n",
    "for s in sentence1:\n",
    "    parsed = spo(s)\n",
    "    sentence1_parsed.append(parsed)\n",
    "sentence1_parsed_test = []\n",
    "for s in sentence1_test:\n",
    "    parsed = spo(s)\n",
    "    sentence1_parsed_test.append(parsed)\n",
    "    \n",
    "#for headlines\n",
    "sentence1_parsed_h = []\n",
    "for s in sentence1_h:\n",
    "    parsed = spo(s)\n",
    "    sentence1_parsed_h.append(parsed)\n",
    "sentence1_parsed_test_h = []\n",
    "for s in sentence1_test_h:\n",
    "    parsed = spo(s)\n",
    "    sentence1_parsed_test_h.append(parsed)\n",
    "    \n",
    "#for images2014\n",
    "sentence1_parsed_4 = []\n",
    "for s in sentence1_4:\n",
    "    parsed = spo(s)\n",
    "    sentence1_parsed_4.append(parsed)\n",
    "sentence1_parsed_test_4 = []\n",
    "for s in sentence1_test_4:\n",
    "    parsed = spo(s)\n",
    "    sentence1_parsed_test_4.append(parsed)\n",
    "\n",
    "#for images2015\n",
    "sentence1_parsed_5 = []\n",
    "for s in sentence1_5:\n",
    "    parsed = spo(s)\n",
    "    sentence1_parsed_5.append(parsed)\n",
    "sentence1_parsed_test_5 = []\n",
    "for s in sentence1_test_5:\n",
    "    parsed = spo(s)\n",
    "    sentence1_parsed_test_5.append(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c00666a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse second sentences in sentence pairs, for both train and test sets\n",
    "sentence2_parsed = []\n",
    "for s in sentence2:\n",
    "    parsed = spo(s)\n",
    "    sentence2_parsed.append(parsed)\n",
    "sentence2_parsed_test = []\n",
    "for s in sentence2_test:\n",
    "    parsed = spo(s)\n",
    "    sentence2_parsed_test.append(parsed)\n",
    "    \n",
    "#for headlines\n",
    "sentence2_parsed_h = []\n",
    "for s in sentence2_h:\n",
    "    parsed = spo(s)\n",
    "    sentence2_parsed_h.append(parsed)\n",
    "sentence2_parsed_test_h = []\n",
    "for s in sentence2_test_h:\n",
    "    parsed = spo(s)\n",
    "    sentence2_parsed_test_h.append(parsed)\n",
    "    \n",
    "#for images2014\n",
    "sentence2_parsed_4 = []\n",
    "for s in sentence2_4:\n",
    "    parsed = spo(s)\n",
    "    sentence2_parsed_4.append(parsed)\n",
    "sentence2_parsed_test_4 = []\n",
    "for s in sentence2_test_4:\n",
    "    parsed = spo(s)\n",
    "    sentence2_parsed_test_4.append(parsed)\n",
    "\n",
    "#for images2015\n",
    "sentence2_parsed_5 = []\n",
    "for s in sentence2_5:\n",
    "    parsed = spo(s)\n",
    "    sentence2_parsed_5.append(parsed)\n",
    "sentence2_parsed_test_5 = []\n",
    "for s in sentence2_test_5:\n",
    "    parsed = spo(s)\n",
    "    sentence2_parsed_test_5.append(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ca61bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S1</th>\n",
       "      <th>P1</th>\n",
       "      <th>O1</th>\n",
       "      <th>S2</th>\n",
       "      <th>P2</th>\n",
       "      <th>O2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>man</td>\n",
       "      <td>playing</td>\n",
       "      <td>flute</td>\n",
       "      <td>man</td>\n",
       "      <td>playing</td>\n",
       "      <td>flute</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>man</td>\n",
       "      <td>spreading</td>\n",
       "      <td>pizza</td>\n",
       "      <td>man</td>\n",
       "      <td>spreading</td>\n",
       "      <td>pizza</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>men</td>\n",
       "      <td>playing</td>\n",
       "      <td>chess</td>\n",
       "      <td>men</td>\n",
       "      <td>playing</td>\n",
       "      <td>chess</td>\n",
       "      <td>2.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>man</td>\n",
       "      <td>playing</td>\n",
       "      <td>cello</td>\n",
       "      <td>man</td>\n",
       "      <td>playing</td>\n",
       "      <td>cello</td>\n",
       "      <td>4.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>man</td>\n",
       "      <td>playing</td>\n",
       "      <td>piano</td>\n",
       "      <td>man</td>\n",
       "      <td>playing</td>\n",
       "      <td>guitar</td>\n",
       "      <td>1.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>man</td>\n",
       "      <td>playing</td>\n",
       "      <td>flute</td>\n",
       "      <td>boy</td>\n",
       "      <td>vacuuming</td>\n",
       "      <td>floor</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>woman</td>\n",
       "      <td>playing</td>\n",
       "      <td>guitar</td>\n",
       "      <td>woman</td>\n",
       "      <td>cuts</td>\n",
       "      <td>meat</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>men</td>\n",
       "      <td>playing</td>\n",
       "      <td>soccer</td>\n",
       "      <td>man</td>\n",
       "      <td>riding</td>\n",
       "      <td>motorcycle</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>woman</td>\n",
       "      <td>running</td>\n",
       "      <td>beach</td>\n",
       "      <td>dog</td>\n",
       "      <td>swimming</td>\n",
       "      <td>pool</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>man</td>\n",
       "      <td>straining</td>\n",
       "      <td>pasta</td>\n",
       "      <td>man</td>\n",
       "      <td>plays</td>\n",
       "      <td>flute</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>798 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        S1         P1      O1     S2         P2          O2  label\n",
       "1      man    playing   flute    man    playing       flute   3.80\n",
       "2      man  spreading   pizza    man  spreading       pizza   3.80\n",
       "3      men    playing   chess    men    playing       chess   2.60\n",
       "4      man    playing   cello    man    playing       cello   4.25\n",
       "7      man    playing   piano    man    playing      guitar   1.60\n",
       "..     ...        ...     ...    ...        ...         ...    ...\n",
       "994    man    playing   flute    boy  vacuuming       floor   0.00\n",
       "995  woman    playing  guitar  woman       cuts        meat   0.75\n",
       "996    men    playing  soccer    man     riding  motorcycle   0.00\n",
       "997  woman    running   beach    dog   swimming        pool   0.00\n",
       "998    man  straining   pasta    man      plays       flute   0.00\n",
       "\n",
       "[798 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S1</th>\n",
       "      <th>P1</th>\n",
       "      <th>O1</th>\n",
       "      <th>S2</th>\n",
       "      <th>P2</th>\n",
       "      <th>O2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>girl</td>\n",
       "      <td>styling</td>\n",
       "      <td>hair</td>\n",
       "      <td>girl</td>\n",
       "      <td>is</td>\n",
       "      <td>hair</td>\n",
       "      <td>2.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>men</td>\n",
       "      <td>play</td>\n",
       "      <td>beach</td>\n",
       "      <td>boys</td>\n",
       "      <td>playing</td>\n",
       "      <td>beach</td>\n",
       "      <td>3.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>woman</td>\n",
       "      <td>measuring</td>\n",
       "      <td>woman</td>\n",
       "      <td>woman</td>\n",
       "      <td>measures</td>\n",
       "      <td>woman</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>man</td>\n",
       "      <td>cutting</td>\n",
       "      <td>cucumber</td>\n",
       "      <td>man</td>\n",
       "      <td>slicing</td>\n",
       "      <td>cucumber</td>\n",
       "      <td>4.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>man</td>\n",
       "      <td>playing</td>\n",
       "      <td>harp</td>\n",
       "      <td>man</td>\n",
       "      <td>playing</td>\n",
       "      <td>keyboard</td>\n",
       "      <td>1.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>swimmers</td>\n",
       "      <td>jump</td>\n",
       "      <td>water</td>\n",
       "      <td>Swimmers</td>\n",
       "      <td>racing</td>\n",
       "      <td>lake</td>\n",
       "      <td>1.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>man</td>\n",
       "      <td>spins</td>\n",
       "      <td>board</td>\n",
       "      <td>man</td>\n",
       "      <td>putting</td>\n",
       "      <td>chicken</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>man</td>\n",
       "      <td>jumps</td>\n",
       "      <td>curb</td>\n",
       "      <td>man</td>\n",
       "      <td>riding</td>\n",
       "      <td>skateboard</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>person</td>\n",
       "      <td>dices</td>\n",
       "      <td>pepper</td>\n",
       "      <td>cook</td>\n",
       "      <td>slicing</td>\n",
       "      <td>peppers</td>\n",
       "      <td>1.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>man</td>\n",
       "      <td>reading</td>\n",
       "      <td>email</td>\n",
       "      <td>person</td>\n",
       "      <td>opening</td>\n",
       "      <td>banana</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           S1         P1        O1        S2        P2          O2  label\n",
       "0        girl    styling      hair      girl        is        hair   2.50\n",
       "1         men       play     beach      boys   playing       beach   3.60\n",
       "2       woman  measuring     woman     woman  measures       woman   5.00\n",
       "3         man    cutting  cucumber       man   slicing    cucumber   4.20\n",
       "4         man    playing      harp       man   playing    keyboard   1.50\n",
       "..        ...        ...       ...       ...       ...         ...    ...\n",
       "244  swimmers       jump     water  Swimmers    racing        lake   1.75\n",
       "245       man      spins     board       man   putting     chicken   0.00\n",
       "246       man      jumps      curb       man    riding  skateboard   2.75\n",
       "247    person      dices    pepper      cook   slicing     peppers   1.80\n",
       "249       man    reading     email    person   opening      banana   0.00\n",
       "\n",
       "[208 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S1</th>\n",
       "      <th>P1</th>\n",
       "      <th>O1</th>\n",
       "      <th>S2</th>\n",
       "      <th>P2</th>\n",
       "      <th>O2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SC</td>\n",
       "      <td>dismisses</td>\n",
       "      <td>case</td>\n",
       "      <td>SC</td>\n",
       "      <td>dismisses</td>\n",
       "      <td>verdict</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Explosion</td>\n",
       "      <td>hits</td>\n",
       "      <td>Syria</td>\n",
       "      <td>Explosion</td>\n",
       "      <td>hits</td>\n",
       "      <td>cities</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Castro</td>\n",
       "      <td>celebrates</td>\n",
       "      <td>birthday</td>\n",
       "      <td>Castro</td>\n",
       "      <td>celebrates</td>\n",
       "      <td>birthday</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Obama</td>\n",
       "      <td>backs</td>\n",
       "      <td>Sea</td>\n",
       "      <td>Obama</td>\n",
       "      <td>calm</td>\n",
       "      <td>Sea</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>China</td>\n",
       "      <td>kills</td>\n",
       "      <td>attack</td>\n",
       "      <td>Teenager</td>\n",
       "      <td>kills</td>\n",
       "      <td>attack</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>mayor</td>\n",
       "      <td>saves</td>\n",
       "      <td>fire</td>\n",
       "      <td>mayor</td>\n",
       "      <td>burning</td>\n",
       "      <td>house</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>Bombs</td>\n",
       "      <td>kill</td>\n",
       "      <td>wound</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>kill</td>\n",
       "      <td>wound</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>lawmakers</td>\n",
       "      <td>take</td>\n",
       "      <td>sanctions</td>\n",
       "      <td>lawmakers</td>\n",
       "      <td>vote</td>\n",
       "      <td>president</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>Obama</td>\n",
       "      <td>need</td>\n",
       "      <td>weapons</td>\n",
       "      <td>gunman</td>\n",
       "      <td>die</td>\n",
       "      <td>hands</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>Sandy</td>\n",
       "      <td>makes</td>\n",
       "      <td>NHC</td>\n",
       "      <td>Sandy</td>\n",
       "      <td>reaches</td>\n",
       "      <td>Jamaica</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>215 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            S1          P1         O1         S2          P2         O2  label\n",
       "1           SC   dismisses       case         SC   dismisses    verdict    4.4\n",
       "2    Explosion        hits      Syria  Explosion        hits     cities    2.6\n",
       "4       Castro  celebrates   birthday     Castro  celebrates   birthday    4.2\n",
       "11       Obama       backs        Sea      Obama        calm        Sea    2.4\n",
       "12       China       kills     attack   Teenager       kills     attack    3.2\n",
       "..         ...         ...        ...        ...         ...        ...    ...\n",
       "580      mayor       saves       fire      mayor     burning      house    4.2\n",
       "585      Bombs        kill      wound   Thailand        kill      wound    3.0\n",
       "586  lawmakers        take  sanctions  lawmakers        vote  president    0.2\n",
       "587      Obama        need    weapons     gunman         die      hands    0.4\n",
       "594      Sandy       makes        NHC      Sandy     reaches    Jamaica    3.0\n",
       "\n",
       "[215 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S1</th>\n",
       "      <th>P1</th>\n",
       "      <th>O1</th>\n",
       "      <th>S2</th>\n",
       "      <th>P2</th>\n",
       "      <th>O2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bomber</td>\n",
       "      <td>kills</td>\n",
       "      <td>church</td>\n",
       "      <td>bomber</td>\n",
       "      <td>kills</td>\n",
       "      <td>Pakistan</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Berri</td>\n",
       "      <td>launch</td>\n",
       "      <td>government</td>\n",
       "      <td>Spain</td>\n",
       "      <td>changed</td>\n",
       "      <td>government</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Nigeria</td>\n",
       "      <td>killed</td>\n",
       "      <td>crash</td>\n",
       "      <td>Nigeria</td>\n",
       "      <td>opens</td>\n",
       "      <td>crash</td>\n",
       "      <td>1.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>airport</td>\n",
       "      <td>evacuated</td>\n",
       "      <td>threat</td>\n",
       "      <td>airport</td>\n",
       "      <td>evacuated</td>\n",
       "      <td>threat</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>forces</td>\n",
       "      <td>kill</td>\n",
       "      <td>protests</td>\n",
       "      <td>forces</td>\n",
       "      <td>kill</td>\n",
       "      <td>Damascus-activists</td>\n",
       "      <td>1.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>party</td>\n",
       "      <td>submits</td>\n",
       "      <td>presidency</td>\n",
       "      <td>party</td>\n",
       "      <td>aims</td>\n",
       "      <td>presidency</td>\n",
       "      <td>3.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Publisher</td>\n",
       "      <td>acquitted</td>\n",
       "      <td>case</td>\n",
       "      <td>PBS</td>\n",
       "      <td>publishes</td>\n",
       "      <td>impartiality</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Strauss</td>\n",
       "      <td>retires</td>\n",
       "      <td>cricket</td>\n",
       "      <td>Strauss</td>\n",
       "      <td>retires</td>\n",
       "      <td>cricket</td>\n",
       "      <td>4.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>forces</td>\n",
       "      <td>shell</td>\n",
       "      <td>rebels</td>\n",
       "      <td>rebels</td>\n",
       "      <td>clash</td>\n",
       "      <td>province</td>\n",
       "      <td>2.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Germany</td>\n",
       "      <td>dodges</td>\n",
       "      <td>quarter</td>\n",
       "      <td>Eurozone</td>\n",
       "      <td>avoids</td>\n",
       "      <td>Germany</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>stocks</td>\n",
       "      <td>eases</td>\n",
       "      <td>fears</td>\n",
       "      <td>Oil</td>\n",
       "      <td>rises</td>\n",
       "      <td>result</td>\n",
       "      <td>1.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>soldier</td>\n",
       "      <td>killed</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>soldiers</td>\n",
       "      <td>killed</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Bomber</td>\n",
       "      <td>Kills</td>\n",
       "      <td>Capital</td>\n",
       "      <td>bomber</td>\n",
       "      <td>kills</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Denver</td>\n",
       "      <td>detonate</td>\n",
       "      <td>suspect</td>\n",
       "      <td>Police</td>\n",
       "      <td>detonate</td>\n",
       "      <td>suspect</td>\n",
       "      <td>4.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Japan</td>\n",
       "      <td>suffers</td>\n",
       "      <td>pictures</td>\n",
       "      <td>wildfires</td>\n",
       "      <td>destroy</td>\n",
       "      <td>pictures</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>violence</td>\n",
       "      <td>amounts</td>\n",
       "      <td>Cross</td>\n",
       "      <td>violence</td>\n",
       "      <td>qualifies</td>\n",
       "      <td>Cross</td>\n",
       "      <td>4.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>mini</td>\n",
       "      <td>makes</td>\n",
       "      <td>debut</td>\n",
       "      <td>mini</td>\n",
       "      <td>makes</td>\n",
       "      <td>Asia</td>\n",
       "      <td>3.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Hamas</td>\n",
       "      <td>hangs</td>\n",
       "      <td>prisoners</td>\n",
       "      <td>Myanmar</td>\n",
       "      <td>announces</td>\n",
       "      <td>prisoners</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Timor</td>\n",
       "      <td>votes</td>\n",
       "      <td>election</td>\n",
       "      <td>Venezuelans</td>\n",
       "      <td>vote</td>\n",
       "      <td>election</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>teachers</td>\n",
       "      <td>strike</td>\n",
       "      <td>dispute</td>\n",
       "      <td>delegates</td>\n",
       "      <td>end</td>\n",
       "      <td>strike</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Afghan</td>\n",
       "      <td>kills</td>\n",
       "      <td>troops</td>\n",
       "      <td>uniform</td>\n",
       "      <td>kills</td>\n",
       "      <td>soldier</td>\n",
       "      <td>3.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Sandy</td>\n",
       "      <td>slams</td>\n",
       "      <td>power</td>\n",
       "      <td>storms</td>\n",
       "      <td>leave</td>\n",
       "      <td>power</td>\n",
       "      <td>2.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Court</td>\n",
       "      <td>Hear</td>\n",
       "      <td>Case</td>\n",
       "      <td>Court</td>\n",
       "      <td>hear</td>\n",
       "      <td>rights</td>\n",
       "      <td>1.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Libya</td>\n",
       "      <td>postpones</td>\n",
       "      <td>July</td>\n",
       "      <td>Libya</td>\n",
       "      <td>postpones</td>\n",
       "      <td>July</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Pakistan</td>\n",
       "      <td>blocks</td>\n",
       "      <td>material</td>\n",
       "      <td>Pakistan</td>\n",
       "      <td>blocks</td>\n",
       "      <td>blasphemy</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           S1         P1           O1           S2         P2  \\\n",
       "1      bomber      kills       church       bomber      kills   \n",
       "4       Berri     launch   government        Spain    changed   \n",
       "7     Nigeria     killed        crash      Nigeria      opens   \n",
       "10    airport  evacuated       threat      airport  evacuated   \n",
       "16     forces       kill     protests       forces       kill   \n",
       "20      party    submits   presidency        party       aims   \n",
       "21  Publisher  acquitted         case          PBS  publishes   \n",
       "25    Strauss    retires      cricket      Strauss    retires   \n",
       "32     forces      shell       rebels       rebels      clash   \n",
       "37    Germany     dodges      quarter     Eurozone     avoids   \n",
       "38     stocks      eases        fears          Oil      rises   \n",
       "39    soldier     killed  Afghanistan     soldiers     killed   \n",
       "41     Bomber      Kills      Capital       bomber      kills   \n",
       "42     Denver   detonate      suspect       Police   detonate   \n",
       "45      Japan    suffers     pictures    wildfires    destroy   \n",
       "47   violence    amounts        Cross     violence  qualifies   \n",
       "49       mini      makes        debut         mini      makes   \n",
       "50      Hamas      hangs    prisoners      Myanmar  announces   \n",
       "51      Timor      votes     election  Venezuelans       vote   \n",
       "54   teachers     strike      dispute    delegates        end   \n",
       "58     Afghan      kills       troops      uniform      kills   \n",
       "64      Sandy      slams        power       storms      leave   \n",
       "66      Court       Hear         Case        Court       hear   \n",
       "67      Libya  postpones         July        Libya  postpones   \n",
       "69   Pakistan     blocks     material     Pakistan     blocks   \n",
       "\n",
       "                    O2  label  \n",
       "1             Pakistan   0.75  \n",
       "4           government   0.40  \n",
       "7                crash   1.60  \n",
       "10              threat   4.00  \n",
       "16  Damascus-activists   1.20  \n",
       "20          presidency   3.60  \n",
       "21        impartiality   0.40  \n",
       "25             cricket   4.80  \n",
       "32            province   2.60  \n",
       "37             Germany   2.00  \n",
       "38              result   1.40  \n",
       "39         Afghanistan   3.00  \n",
       "41         Afghanistan   2.00  \n",
       "42             suspect   4.60  \n",
       "45            pictures   0.80  \n",
       "47               Cross   4.40  \n",
       "49                Asia   3.40  \n",
       "50           prisoners   0.60  \n",
       "51            election   1.00  \n",
       "54              strike   2.40  \n",
       "58             soldier   3.40  \n",
       "64               power   2.60  \n",
       "66              rights   1.40  \n",
       "67                July   4.00  \n",
       "69           blasphemy   3.80  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S1</th>\n",
       "      <th>P1</th>\n",
       "      <th>O1</th>\n",
       "      <th>S2</th>\n",
       "      <th>P2</th>\n",
       "      <th>O2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eyes</td>\n",
       "      <td>standing</td>\n",
       "      <td>chair</td>\n",
       "      <td>cat</td>\n",
       "      <td>stands</td>\n",
       "      <td>floor</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bus</td>\n",
       "      <td>driving</td>\n",
       "      <td>street</td>\n",
       "      <td>bus</td>\n",
       "      <td>driving</td>\n",
       "      <td>street</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train</td>\n",
       "      <td>waiting</td>\n",
       "      <td>station</td>\n",
       "      <td>train</td>\n",
       "      <td>sits</td>\n",
       "      <td>station</td>\n",
       "      <td>4.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>computer</td>\n",
       "      <td>sitting</td>\n",
       "      <td>floor</td>\n",
       "      <td>computer</td>\n",
       "      <td>sitting</td>\n",
       "      <td>floor</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>people</td>\n",
       "      <td>eat</td>\n",
       "      <td>outside</td>\n",
       "      <td>people</td>\n",
       "      <td>sitting</td>\n",
       "      <td>table</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>bird</td>\n",
       "      <td>perched</td>\n",
       "      <td>hand</td>\n",
       "      <td>bird</td>\n",
       "      <td>perched</td>\n",
       "      <td>person</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>baby</td>\n",
       "      <td>holds</td>\n",
       "      <td>packet</td>\n",
       "      <td>highchair</td>\n",
       "      <td>holds</td>\n",
       "      <td>packet</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>cat</td>\n",
       "      <td>looking</td>\n",
       "      <td>camera</td>\n",
       "      <td>dog</td>\n",
       "      <td>looking</td>\n",
       "      <td>camera</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>bird</td>\n",
       "      <td>sitting</td>\n",
       "      <td>ground</td>\n",
       "      <td>bird</td>\n",
       "      <td>sitting</td>\n",
       "      <td>branch</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>birds</td>\n",
       "      <td>hanging</td>\n",
       "      <td>tree</td>\n",
       "      <td>bird</td>\n",
       "      <td>perched</td>\n",
       "      <td>tree</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>243 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           S1        P1       O1         S2       P2       O2  label\n",
       "1        eyes  standing    chair        cat   stands    floor    2.6\n",
       "3         bus   driving   street        bus  driving   street    4.0\n",
       "4       train   waiting  station      train     sits  station    4.8\n",
       "6    computer   sitting    floor   computer  sitting    floor    3.8\n",
       "8      people       eat  outside     people  sitting    table    2.8\n",
       "..        ...       ...      ...        ...      ...      ...    ...\n",
       "489      bird   perched     hand       bird  perched   person    4.4\n",
       "490      baby     holds   packet  highchair    holds   packet    4.0\n",
       "494       cat   looking   camera        dog  looking   camera    0.8\n",
       "495      bird   sitting   ground       bird  sitting   branch    2.5\n",
       "496     birds   hanging     tree       bird  perched     tree    2.0\n",
       "\n",
       "[243 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S1</th>\n",
       "      <th>P1</th>\n",
       "      <th>O1</th>\n",
       "      <th>S2</th>\n",
       "      <th>P2</th>\n",
       "      <th>O2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>boys</td>\n",
       "      <td>look</td>\n",
       "      <td>shed</td>\n",
       "      <td>area</td>\n",
       "      <td>looking</td>\n",
       "      <td>shed</td>\n",
       "      <td>2.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>man</td>\n",
       "      <td>sleeps</td>\n",
       "      <td>lap</td>\n",
       "      <td>chair</td>\n",
       "      <td>holding</td>\n",
       "      <td>baby</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cats</td>\n",
       "      <td>looking</td>\n",
       "      <td>window</td>\n",
       "      <td>cat</td>\n",
       "      <td>looking</td>\n",
       "      <td>window</td>\n",
       "      <td>2.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>person</td>\n",
       "      <td>wearing</td>\n",
       "      <td>structure</td>\n",
       "      <td>boots</td>\n",
       "      <td>standing</td>\n",
       "      <td>motorcycle</td>\n",
       "      <td>1.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>helmet</td>\n",
       "      <td>sits</td>\n",
       "      <td>bicycle</td>\n",
       "      <td>girl</td>\n",
       "      <td>wearing</td>\n",
       "      <td>background</td>\n",
       "      <td>3.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>gentleman</td>\n",
       "      <td>looking</td>\n",
       "      <td>motorcycle</td>\n",
       "      <td>man</td>\n",
       "      <td>looking</td>\n",
       "      <td>motorcycle</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>airplane</td>\n",
       "      <td>parked</td>\n",
       "      <td>grass</td>\n",
       "      <td>shirt</td>\n",
       "      <td>tied</td>\n",
       "      <td>railing</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>people</td>\n",
       "      <td>walking</td>\n",
       "      <td>mushroom</td>\n",
       "      <td>people</td>\n",
       "      <td>walking</td>\n",
       "      <td>mushroom</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>cows</td>\n",
       "      <td>look</td>\n",
       "      <td>camera</td>\n",
       "      <td>sink</td>\n",
       "      <td>looking</td>\n",
       "      <td>camera</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>woman</td>\n",
       "      <td>holding</td>\n",
       "      <td>baby</td>\n",
       "      <td>woman</td>\n",
       "      <td>holding</td>\n",
       "      <td>baby</td>\n",
       "      <td>3.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            S1       P1          O1      S2        P2          O2  label\n",
       "0         boys     look        shed    area   looking        shed   2.80\n",
       "2          man   sleeps         lap   chair   holding        baby   4.00\n",
       "3         cats  looking      window     cat   looking      window   2.60\n",
       "4       person  wearing   structure   boots  standing  motorcycle   1.20\n",
       "7       helmet     sits     bicycle    girl   wearing  background   3.40\n",
       "..         ...      ...         ...     ...       ...         ...    ...\n",
       "125  gentleman  looking  motorcycle     man   looking  motorcycle   4.00\n",
       "127   airplane   parked       grass   shirt      tied     railing   0.00\n",
       "130     people  walking    mushroom  people   walking    mushroom   4.00\n",
       "133       cows     look      camera    sink   looking      camera   0.75\n",
       "134      woman  holding        baby   woman   holding        baby   3.40\n",
       "\n",
       "[78 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S1</th>\n",
       "      <th>P1</th>\n",
       "      <th>O1</th>\n",
       "      <th>S2</th>\n",
       "      <th>P2</th>\n",
       "      <th>O2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dogs</td>\n",
       "      <td>playing</td>\n",
       "      <td>ball</td>\n",
       "      <td>dogs</td>\n",
       "      <td>play</td>\n",
       "      <td>football</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>children</td>\n",
       "      <td>stand</td>\n",
       "      <td>fence</td>\n",
       "      <td>children</td>\n",
       "      <td>standing</td>\n",
       "      <td>fence</td>\n",
       "      <td>4.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog</td>\n",
       "      <td>emerges</td>\n",
       "      <td>mouth</td>\n",
       "      <td>dog</td>\n",
       "      <td>walking</td>\n",
       "      <td>mouth</td>\n",
       "      <td>4.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dog</td>\n",
       "      <td>swims</td>\n",
       "      <td>pool</td>\n",
       "      <td>dog</td>\n",
       "      <td>swimming</td>\n",
       "      <td>pool</td>\n",
       "      <td>4.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dog</td>\n",
       "      <td>runs</td>\n",
       "      <td>woods</td>\n",
       "      <td>dog</td>\n",
       "      <td>running</td>\n",
       "      <td>woods</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>person</td>\n",
       "      <td>squeezing</td>\n",
       "      <td>face</td>\n",
       "      <td>dog</td>\n",
       "      <td>running</td>\n",
       "      <td>area</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>Dog</td>\n",
       "      <td>running</td>\n",
       "      <td>mouth</td>\n",
       "      <td>dog</td>\n",
       "      <td>swims</td>\n",
       "      <td>mouth</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>girl</td>\n",
       "      <td>running</td>\n",
       "      <td>beach</td>\n",
       "      <td>girl</td>\n",
       "      <td>running</td>\n",
       "      <td>beach</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>player</td>\n",
       "      <td>throws</td>\n",
       "      <td>ball</td>\n",
       "      <td>player</td>\n",
       "      <td>holds</td>\n",
       "      <td>ball</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>girl</td>\n",
       "      <td>walking</td>\n",
       "      <td>sidewalk</td>\n",
       "      <td>man</td>\n",
       "      <td>walking</td>\n",
       "      <td>streets</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>356 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           S1         P1        O1        S2        P2        O2  label\n",
       "0        dogs    playing      ball      dogs      play  football    4.4\n",
       "2    children      stand     fence  children  standing     fence    4.6\n",
       "3         dog    emerges     mouth       dog   walking     mouth    4.8\n",
       "6         dog      swims      pool       dog  swimming      pool    4.6\n",
       "8         dog       runs     woods       dog   running     woods    4.0\n",
       "..        ...        ...       ...       ...       ...       ...    ...\n",
       "497    person  squeezing      face       dog   running      area    0.0\n",
       "498       Dog    running     mouth       dog     swims     mouth    2.0\n",
       "499      girl    running     beach      girl   running     beach    4.2\n",
       "500    player     throws      ball    player     holds      ball    1.6\n",
       "502      girl    walking  sidewalk       man   walking   streets    1.0\n",
       "\n",
       "[356 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S1</th>\n",
       "      <th>P1</th>\n",
       "      <th>O1</th>\n",
       "      <th>S2</th>\n",
       "      <th>P2</th>\n",
       "      <th>O2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dogs</td>\n",
       "      <td>play</td>\n",
       "      <td>grass</td>\n",
       "      <td>dogs</td>\n",
       "      <td>playing</td>\n",
       "      <td>snow</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>building</td>\n",
       "      <td>painted</td>\n",
       "      <td>jack</td>\n",
       "      <td>building</td>\n",
       "      <td>painted</td>\n",
       "      <td>Jack</td>\n",
       "      <td>4.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dog</td>\n",
       "      <td>swims</td>\n",
       "      <td>water</td>\n",
       "      <td>cape</td>\n",
       "      <td>running</td>\n",
       "      <td>snow</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dog</td>\n",
       "      <td>runs</td>\n",
       "      <td>mouth</td>\n",
       "      <td>dogs</td>\n",
       "      <td>play</td>\n",
       "      <td>skyline</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bird</td>\n",
       "      <td>flies</td>\n",
       "      <td>water</td>\n",
       "      <td>bird</td>\n",
       "      <td>flies</td>\n",
       "      <td>water</td>\n",
       "      <td>4.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>dog</td>\n",
       "      <td>runs</td>\n",
       "      <td>water</td>\n",
       "      <td>dog</td>\n",
       "      <td>runs</td>\n",
       "      <td>field</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>girl</td>\n",
       "      <td>running</td>\n",
       "      <td>path</td>\n",
       "      <td>girl</td>\n",
       "      <td>talking</td>\n",
       "      <td>phone</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>man</td>\n",
       "      <td>colored</td>\n",
       "      <td>houses</td>\n",
       "      <td>boat</td>\n",
       "      <td>passes</td>\n",
       "      <td>houses</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>sunglasses</td>\n",
       "      <td>holds</td>\n",
       "      <td>hands</td>\n",
       "      <td>top</td>\n",
       "      <td>blowing</td>\n",
       "      <td>bubble</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>deer</td>\n",
       "      <td>jumps</td>\n",
       "      <td>fence</td>\n",
       "      <td>deer</td>\n",
       "      <td>jumping</td>\n",
       "      <td>fence</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             S1       P1      O1        S2       P2       O2  label\n",
       "0          dogs     play   grass      dogs  playing     snow    2.8\n",
       "1      building  painted    jack  building  painted     Jack    4.8\n",
       "2           dog    swims   water      cape  running     snow    1.4\n",
       "4           dog     runs   mouth      dogs     play  skyline    1.8\n",
       "5          bird    flies   water      bird    flies    water    4.8\n",
       "..          ...      ...     ...       ...      ...      ...    ...\n",
       "110         dog     runs   water       dog     runs    field    2.0\n",
       "111        girl  running    path      girl  talking    phone    0.8\n",
       "112         man  colored  houses      boat   passes   houses    5.0\n",
       "113  sunglasses    holds   hands       top  blowing   bubble    1.0\n",
       "114        deer    jumps   fence      deer  jumping    fence    5.0\n",
       "\n",
       "[85 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#sentences where SPO could not parse out any of the subject, predicate, or object are meaningless to us\n",
    "#Since accurate comparisons cannot be made, remove any pairs affected by this\n",
    "import pandas as pd\n",
    "#MSRvid data\n",
    "df1 = pd.DataFrame(sentence1_parsed, columns = [\"S1\", \"P1\", \"O1\"])\n",
    "df2 = pd.DataFrame(sentence2_parsed, columns = [\"S2\", \"P2\", \"O2\"])\n",
    "df3 = pd.DataFrame(labels, columns = [\"label\"])\n",
    "combined_df = df1.join(df2)\n",
    "combined_df = combined_df.join(df3)\n",
    "cleaned_df = combined_df[(combined_df.S1 != '') & (combined_df.P1 != '') & (combined_df.O1 != '')]\n",
    "cleaned_df = cleaned_df[(cleaned_df.S2 != '') & (cleaned_df.P2 != '') & (cleaned_df.O2 != '')]\n",
    "display(cleaned_df)\n",
    "#now split these back out into separate lists; still need to process those via Word2Vec\n",
    "df1 = cleaned_df.iloc[:,:3]\n",
    "df2 = cleaned_df.iloc[:,3:6]\n",
    "df3 = cleaned_df.iloc[:,6:]\n",
    "sentence1_cleaned = df1.values.tolist()\n",
    "sentence2_cleaned = df2.values.tolist()\n",
    "labels = df3.values.tolist()\n",
    "#now do the same thing for test data\n",
    "df1 = pd.DataFrame(sentence1_parsed_test, columns = [\"S1\", \"P1\", \"O1\"])\n",
    "df2 = pd.DataFrame(sentence2_parsed_test, columns = [\"S2\", \"P2\", \"O2\"])\n",
    "df3 = pd.DataFrame(labels_test, columns = [\"label\"])\n",
    "combined_df = df1.join(df2)\n",
    "combined_df = combined_df.join(df3)\n",
    "cleaned_df = combined_df[(combined_df.S1 != '') & (combined_df.P1 != '') & (combined_df.O1 != '')]\n",
    "cleaned_df = cleaned_df[(cleaned_df.S2 != '') & (cleaned_df.P2 != '') & (cleaned_df.O2 != '')]\n",
    "display(cleaned_df)\n",
    "#now split these back out into separate lists; still need to process those via Word2Vec\n",
    "df1 = cleaned_df.iloc[:,:3]\n",
    "df2 = cleaned_df.iloc[:,3:6]\n",
    "df3 = cleaned_df.iloc[:,6:]\n",
    "sentence1_cleaned_test = df1.values.tolist()\n",
    "sentence2_cleaned_test = df2.values.tolist()\n",
    "labels_test = df3.values.tolist()\n",
    "\n",
    "#headlines data\n",
    "df1 = pd.DataFrame(sentence1_parsed_h, columns = [\"S1\", \"P1\", \"O1\"])\n",
    "df2 = pd.DataFrame(sentence2_parsed_h, columns = [\"S2\", \"P2\", \"O2\"])\n",
    "df3 = pd.DataFrame(labels_h, columns = [\"label\"])\n",
    "combined_df = df1.join(df2)\n",
    "combined_df = combined_df.join(df3)\n",
    "cleaned_df = combined_df[(combined_df.S1 != '') & (combined_df.P1 != '') & (combined_df.O1 != '')]\n",
    "cleaned_df = cleaned_df[(cleaned_df.S2 != '') & (cleaned_df.P2 != '') & (cleaned_df.O2 != '')]\n",
    "display(cleaned_df)\n",
    "#now split these back out into separate lists; still need to process those via Word2Vec\n",
    "df1 = cleaned_df.iloc[:,:3]\n",
    "df2 = cleaned_df.iloc[:,3:6]\n",
    "df3 = cleaned_df.iloc[:,6:]\n",
    "sentence1_cleaned_h = df1.values.tolist()\n",
    "sentence2_cleaned_h = df2.values.tolist()\n",
    "labels_h = df3.values.tolist()\n",
    "#now do the same thing for test data\n",
    "df1 = pd.DataFrame(sentence1_parsed_test_h, columns = [\"S1\", \"P1\", \"O1\"])\n",
    "df2 = pd.DataFrame(sentence2_parsed_test_h, columns = [\"S2\", \"P2\", \"O2\"])\n",
    "df3 = pd.DataFrame(labels_test_h, columns = [\"label\"])\n",
    "combined_df = df1.join(df2)\n",
    "combined_df = combined_df.join(df3)\n",
    "cleaned_df = combined_df[(combined_df.S1 != '') & (combined_df.P1 != '') & (combined_df.O1 != '')]\n",
    "cleaned_df = cleaned_df[(cleaned_df.S2 != '') & (cleaned_df.P2 != '') & (cleaned_df.O2 != '')]\n",
    "display(cleaned_df)\n",
    "#now split these back out into separate lists; still need to process those via Word2Vec\n",
    "df1 = cleaned_df.iloc[:,:3]\n",
    "df2 = cleaned_df.iloc[:,3:6]\n",
    "df3 = cleaned_df.iloc[:,6:]\n",
    "sentence1_cleaned_test_h = df1.values.tolist()\n",
    "sentence2_cleaned_test_h = df2.values.tolist()\n",
    "labels_test_h = df3.values.tolist()\n",
    "\n",
    "#images2014 data\n",
    "df1 = pd.DataFrame(sentence1_parsed_4, columns = [\"S1\", \"P1\", \"O1\"])\n",
    "df2 = pd.DataFrame(sentence2_parsed_4, columns = [\"S2\", \"P2\", \"O2\"])\n",
    "df3 = pd.DataFrame(labels_4, columns = [\"label\"])\n",
    "combined_df = df1.join(df2)\n",
    "combined_df = combined_df.join(df3)\n",
    "cleaned_df = combined_df[(combined_df.S1 != '') & (combined_df.P1 != '') & (combined_df.O1 != '')]\n",
    "cleaned_df = cleaned_df[(cleaned_df.S2 != '') & (cleaned_df.P2 != '') & (cleaned_df.O2 != '')]\n",
    "display(cleaned_df)\n",
    "#now split these back out into separate lists; still need to process those via Word2Vec\n",
    "df1 = cleaned_df.iloc[:,:3]\n",
    "df2 = cleaned_df.iloc[:,3:6]\n",
    "df3 = cleaned_df.iloc[:,6:]\n",
    "sentence1_cleaned_4 = df1.values.tolist()\n",
    "sentence2_cleaned_4 = df2.values.tolist()\n",
    "labels_4 = df3.values.tolist()\n",
    "#now do the same thing for test data\n",
    "df1 = pd.DataFrame(sentence1_parsed_test_4, columns = [\"S1\", \"P1\", \"O1\"])\n",
    "df2 = pd.DataFrame(sentence2_parsed_test_4, columns = [\"S2\", \"P2\", \"O2\"])\n",
    "df3 = pd.DataFrame(labels_test_4, columns = [\"label\"])\n",
    "combined_df = df1.join(df2)\n",
    "combined_df = combined_df.join(df3)\n",
    "cleaned_df = combined_df[(combined_df.S1 != '') & (combined_df.P1 != '') & (combined_df.O1 != '')]\n",
    "cleaned_df = cleaned_df[(cleaned_df.S2 != '') & (cleaned_df.P2 != '') & (cleaned_df.O2 != '')]\n",
    "display(cleaned_df)\n",
    "#now split these back out into separate lists; still need to process those via Word2Vec\n",
    "df1 = cleaned_df.iloc[:,:3]\n",
    "df2 = cleaned_df.iloc[:,3:6]\n",
    "df3 = cleaned_df.iloc[:,6:]\n",
    "sentence1_cleaned_test_4 = df1.values.tolist()\n",
    "sentence2_cleaned_test_4 = df2.values.tolist()\n",
    "labels_test_4 = df3.values.tolist()\n",
    "\n",
    "#images2015 data\n",
    "df1 = pd.DataFrame(sentence1_parsed_5, columns = [\"S1\", \"P1\", \"O1\"])\n",
    "df2 = pd.DataFrame(sentence2_parsed_5, columns = [\"S2\", \"P2\", \"O2\"])\n",
    "df3 = pd.DataFrame(labels_5, columns = [\"label\"])\n",
    "combined_df = df1.join(df2)\n",
    "combined_df = combined_df.join(df3)\n",
    "cleaned_df = combined_df[(combined_df.S1 != '') & (combined_df.P1 != '') & (combined_df.O1 != '')]\n",
    "cleaned_df = cleaned_df[(cleaned_df.S2 != '') & (cleaned_df.P2 != '') & (cleaned_df.O2 != '')]\n",
    "display(cleaned_df)\n",
    "#now split these back out into separate lists; still need to process those via Word2Vec\n",
    "df1 = cleaned_df.iloc[:,:3]\n",
    "df2 = cleaned_df.iloc[:,3:6]\n",
    "df3 = cleaned_df.iloc[:,6:]\n",
    "sentence1_cleaned_5 = df1.values.tolist()\n",
    "sentence2_cleaned_5 = df2.values.tolist()\n",
    "labels_5 = df3.values.tolist()\n",
    "#now do the same thing for test data\n",
    "df1 = pd.DataFrame(sentence1_parsed_test_5, columns = [\"S1\", \"P1\", \"O1\"])\n",
    "df2 = pd.DataFrame(sentence2_parsed_test_5, columns = [\"S2\", \"P2\", \"O2\"])\n",
    "df3 = pd.DataFrame(labels_test_5, columns = [\"label\"])\n",
    "combined_df = df1.join(df2)\n",
    "combined_df = combined_df.join(df3)\n",
    "cleaned_df = combined_df[(combined_df.S1 != '') & (combined_df.P1 != '') & (combined_df.O1 != '')]\n",
    "cleaned_df = cleaned_df[(cleaned_df.S2 != '') & (cleaned_df.P2 != '') & (cleaned_df.O2 != '')]\n",
    "display(cleaned_df)\n",
    "#now split these back out into separate lists; still need to process those via Word2Vec\n",
    "df1 = cleaned_df.iloc[:,:3]\n",
    "df2 = cleaned_df.iloc[:,3:6]\n",
    "df3 = cleaned_df.iloc[:,6:]\n",
    "sentence1_cleaned_test_5 = df1.values.tolist()\n",
    "sentence2_cleaned_test_5 = df2.values.tolist()\n",
    "labels_test_5 = df3.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8eac059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2Vec conversion. Dimensions and procedure match what's in the paper\n",
    "#Point of ambiguity: we only ever have a single word for a subject, predicate, or object; but paper seems to suggest sometimes\n",
    "#that there can be multi-word subjects/predicates/objects\n",
    "import os\n",
    "import numpy as np\n",
    "RANDOM_SEED = 23432098\n",
    "os.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "#MSRvid data\n",
    "#build a Word2Vec model for train data and one for test data\n",
    "train_sentences = sentence1_cleaned + sentence2_cleaned\n",
    "test_sentences = sentence1_cleaned_test + sentence2_cleaned_test\n",
    "w2v1 = Word2Vec(train_sentences, vector_size=50, workers=1, min_count=1)\n",
    "w2v1_test = Word2Vec(test_sentences, vector_size=50, workers=1, min_count=1)\n",
    "#then to get the sentences_final, pull out the .wv for each word in the sentence and transpose it\n",
    "sentence1_final = []\n",
    "sentence2_final = []\n",
    "sentence1_final_test = []\n",
    "sentence2_final_test = []\n",
    "for s in sentence1_cleaned:\n",
    "    words = []\n",
    "    for w in s:\n",
    "        mat = w2v1.wv[w]\n",
    "        words.append(mat.transpose())\n",
    "    sentence1_final.append(words)\n",
    "for s in sentence2_cleaned:\n",
    "    words = []\n",
    "    for w in s:\n",
    "        mat = w2v1.wv[w]\n",
    "        words.append(mat.transpose())\n",
    "    sentence2_final.append(words)\n",
    "for s in sentence1_cleaned_test:\n",
    "    words = []\n",
    "    for w in s:\n",
    "        mat = w2v1_test.wv[w]\n",
    "        words.append(mat.transpose())\n",
    "    sentence1_final_test.append(words)\n",
    "for s in sentence2_cleaned_test:\n",
    "    words = []\n",
    "    for w in s:\n",
    "        mat = w2v1_test.wv[w]\n",
    "        words.append(mat.transpose())\n",
    "    sentence2_final_test.append(words)\n",
    "    \n",
    "#headlines data\n",
    "#build a Word2Vec model for train data and one for test data\n",
    "train_sentences = sentence1_cleaned_h + sentence2_cleaned_h\n",
    "test_sentences = sentence1_cleaned_test_h + sentence2_cleaned_test_h\n",
    "w2v1 = Word2Vec(train_sentences, vector_size=50, workers=1, min_count=1)\n",
    "w2v1_test = Word2Vec(test_sentences, vector_size=50, workers=1, min_count=1)\n",
    "#then to get the sentences_final, pull out the .wv for each word in the sentence and transpose it\n",
    "sentence1_final_h = []\n",
    "sentence2_final_h = []\n",
    "sentence1_final_test_h = []\n",
    "sentence2_final_test_h = []\n",
    "for s in sentence1_cleaned_h:\n",
    "    words = []\n",
    "    for w in s:\n",
    "        mat = w2v1.wv[w]\n",
    "        words.append(mat.transpose())\n",
    "    sentence1_final_h.append(words)\n",
    "for s in sentence2_cleaned_h:\n",
    "    words = []\n",
    "    for w in s:\n",
    "        mat = w2v1.wv[w]\n",
    "        words.append(mat.transpose())\n",
    "    sentence2_final_h.append(words)\n",
    "for s in sentence1_cleaned_test_h:\n",
    "    words = []\n",
    "    for w in s:\n",
    "        mat = w2v1_test.wv[w]\n",
    "        words.append(mat.transpose())\n",
    "    sentence1_final_test_h.append(words)\n",
    "for s in sentence2_cleaned_test_h:\n",
    "    words = []\n",
    "    for w in s:\n",
    "        mat = w2v1_test.wv[w]\n",
    "        words.append(mat.transpose())\n",
    "    sentence2_final_test_h.append(words)\n",
    "    \n",
    "#images2014 data\n",
    "#build a Word2Vec model for train data and one for test data\n",
    "train_sentences = sentence1_cleaned_4 + sentence2_cleaned_4\n",
    "test_sentences = sentence1_cleaned_test_4 + sentence2_cleaned_test_4\n",
    "w2v1 = Word2Vec(train_sentences, vector_size=50, workers=1, min_count=1)\n",
    "w2v1_test = Word2Vec(test_sentences, vector_size=50, workers=1, min_count=1)\n",
    "#then to get the sentences_final, pull out the .wv for each word in the sentence and transpose it\n",
    "sentence1_final_4 = []\n",
    "sentence2_final_4 = []\n",
    "sentence1_final_test_4 = []\n",
    "sentence2_final_test_4 = []\n",
    "for s in sentence1_cleaned_4:\n",
    "    words = []\n",
    "    for w in s:\n",
    "        mat = w2v1.wv[w]\n",
    "        words.append(mat.transpose())\n",
    "    sentence1_final_4.append(words)\n",
    "for s in sentence2_cleaned_4:\n",
    "    words = []\n",
    "    for w in s:\n",
    "        mat = w2v1.wv[w]\n",
    "        words.append(mat.transpose())\n",
    "    sentence2_final_4.append(words)\n",
    "for s in sentence1_cleaned_test_4:\n",
    "    words = []\n",
    "    for w in s:\n",
    "        mat = w2v1_test.wv[w]\n",
    "        words.append(mat.transpose())\n",
    "    sentence1_final_test_4.append(words)\n",
    "for s in sentence2_cleaned_test_4:\n",
    "    words = []\n",
    "    for w in s:\n",
    "        mat = w2v1_test.wv[w]\n",
    "        words.append(mat.transpose())\n",
    "    sentence2_final_test_4.append(words)\n",
    "\n",
    "#images2015 data\n",
    "#build a Word2Vec model for train data and one for test data\n",
    "train_sentences = sentence1_cleaned_5 + sentence2_cleaned_5\n",
    "test_sentences = sentence1_cleaned_test_5 + sentence2_cleaned_test_5\n",
    "w2v1 = Word2Vec(train_sentences, vector_size=50, workers=1, min_count=1)\n",
    "w2v1_test = Word2Vec(test_sentences, vector_size=50, workers=1, min_count=1)\n",
    "#then to get the sentences_final, pull out the .wv for each word in the sentence and transpose it\n",
    "sentence1_final_5 = []\n",
    "sentence2_final_5 = []\n",
    "sentence1_final_test_5 = []\n",
    "sentence2_final_test_5 = []\n",
    "for s in sentence1_cleaned_5:\n",
    "    words = []\n",
    "    for w in s:\n",
    "        mat = w2v1.wv[w]\n",
    "        words.append(mat.transpose())\n",
    "    sentence1_final_5.append(words)\n",
    "for s in sentence2_cleaned_5:\n",
    "    words = []\n",
    "    for w in s:\n",
    "        mat = w2v1.wv[w]\n",
    "        words.append(mat.transpose())\n",
    "    sentence2_final_5.append(words)\n",
    "for s in sentence1_cleaned_test_5:\n",
    "    words = []\n",
    "    for w in s:\n",
    "        mat = w2v1_test.wv[w]\n",
    "        words.append(mat.transpose())\n",
    "    sentence1_final_test_5.append(words)\n",
    "for s in sentence2_cleaned_test_5:\n",
    "    words = []\n",
    "    for w in s:\n",
    "        mat = w2v1_test.wv[w]\n",
    "        words.append(mat.transpose())\n",
    "    sentence2_final_test_5.append(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d5322d",
   "metadata": {},
   "source": [
    "## Model Definition and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c3d88fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the CNN model\n",
    "#I am using MaxPool2d as opposed to k-max pooling as we know the sentences should always be the same size\n",
    "#I have left the print statements intact so you can see how the values become smaller/closer to zero\n",
    "#just uncomment, then run to see\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(50, 17, kernel_size=(3,3), padding=1)\n",
    "        self.conv2 = nn.Conv2d(17, 6, kernel_size=(3,3), padding=1)\n",
    "        #self.pool1 = nn.MaxPool2d(3) #errors if set to 3 as the original paper uses for its pooling\n",
    "        self.pool2 = nn.MaxPool2d(1)\n",
    "        self.fc1 = nn.Linear(18, 1)\n",
    "\n",
    "    def forward(self, x_prime):\n",
    "        x_prime = F.relu(self.conv1(x_prime))\n",
    "        #print(\"After first conv layer:\")\n",
    "        #print(x_prime)\n",
    "        x_prime = F.relu(self.conv2(x_prime))\n",
    "        #print(\"After second conv layer:\")\n",
    "        #print(x_prime)\n",
    "        x_prime = self.pool2(x_prime)\n",
    "        #print(\"After first pool:\")\n",
    "        #print(x_prime)\n",
    "        x_prime = self.pool2(x_prime)\n",
    "        #print(\"After second pool:\")\n",
    "        #print(x_prime)\n",
    "        x_prime = x_prime.view(-1, 18)\n",
    "        #print(\"View X prime:\")\n",
    "        #print(x_prime)\n",
    "        x_prime = self.fc1(x_prime)\n",
    "        #print(\"After FC layer:\")\n",
    "        #print(x_prime) #show what is being output from the model\n",
    "        return x_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd5aeb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the data loaders\n",
    "#to do this, construct the training data by binding together final sentence1 and sentence2 with their target score\n",
    "#MSRvid data\n",
    "df1 = pd.DataFrame(sentence1_final, columns = [\"S1\", \"P1\", \"O1\"])\n",
    "df2 = pd.DataFrame(sentence2_final, columns = [\"S2\", \"P2\", \"O2\"])\n",
    "df3 = pd.DataFrame(labels, columns = [\"label\"])\n",
    "combined_df = df1.join(df2)\n",
    "combined_df = combined_df.join(df3)\n",
    "train_data = combined_df.values.tolist()\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "#repeat for test data\n",
    "df1 = pd.DataFrame(sentence1_final_test, columns = [\"S1\", \"P1\", \"O1\"])\n",
    "df2 = pd.DataFrame(sentence2_final_test, columns = [\"S2\", \"P2\", \"O2\"])\n",
    "df3 = pd.DataFrame(labels_test, columns = [\"label\"])\n",
    "combined_df = df1.join(df2)\n",
    "combined_df = combined_df.join(df3)\n",
    "test_data = combined_df.values.tolist()\n",
    "val_loader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "#headlines data\n",
    "df1 = pd.DataFrame(sentence1_final_h, columns = [\"S1\", \"P1\", \"O1\"])\n",
    "df2 = pd.DataFrame(sentence2_final_h, columns = [\"S2\", \"P2\", \"O2\"])\n",
    "df3 = pd.DataFrame(labels_h, columns = [\"label\"])\n",
    "combined_df = df1.join(df2)\n",
    "combined_df = combined_df.join(df3)\n",
    "train_data_h = combined_df.values.tolist()\n",
    "train_loader_h = torch.utils.data.DataLoader(train_data_h, batch_size=64, shuffle=True)\n",
    "#repeat for test data\n",
    "df1 = pd.DataFrame(sentence1_final_test_h, columns = [\"S1\", \"P1\", \"O1\"])\n",
    "df2 = pd.DataFrame(sentence2_final_test_h, columns = [\"S2\", \"P2\", \"O2\"])\n",
    "df3 = pd.DataFrame(labels_test_h, columns = [\"label\"])\n",
    "combined_df = df1.join(df2)\n",
    "combined_df = combined_df.join(df3)\n",
    "test_data_h = combined_df.values.tolist()\n",
    "val_loader_h = torch.utils.data.DataLoader(test_data_h, batch_size=64, shuffle=False)\n",
    "\n",
    "#images2014 data\n",
    "df1 = pd.DataFrame(sentence1_final_4, columns = [\"S1\", \"P1\", \"O1\"])\n",
    "df2 = pd.DataFrame(sentence2_final_4, columns = [\"S2\", \"P2\", \"O2\"])\n",
    "df3 = pd.DataFrame(labels_4, columns = [\"label\"])\n",
    "combined_df = df1.join(df2)\n",
    "combined_df = combined_df.join(df3)\n",
    "train_data_4 = combined_df.values.tolist()\n",
    "train_loader_4 = torch.utils.data.DataLoader(train_data_4, batch_size=64, shuffle=True)\n",
    "#repeat for test data\n",
    "df1 = pd.DataFrame(sentence1_final_test_4, columns = [\"S1\", \"P1\", \"O1\"])\n",
    "df2 = pd.DataFrame(sentence2_final_test_4, columns = [\"S2\", \"P2\", \"O2\"])\n",
    "df3 = pd.DataFrame(labels_test_4, columns = [\"label\"])\n",
    "combined_df = df1.join(df2)\n",
    "combined_df = combined_df.join(df3)\n",
    "test_data_4 = combined_df.values.tolist()\n",
    "val_loader_4 = torch.utils.data.DataLoader(test_data_4, batch_size=64, shuffle=False)\n",
    "\n",
    "#images2015 data\n",
    "df1 = pd.DataFrame(sentence1_final_5, columns = [\"S1\", \"P1\", \"O1\"])\n",
    "df2 = pd.DataFrame(sentence2_final_5, columns = [\"S2\", \"P2\", \"O2\"])\n",
    "df3 = pd.DataFrame(labels_5, columns = [\"label\"])\n",
    "combined_df = df1.join(df2)\n",
    "combined_df = combined_df.join(df3)\n",
    "train_data_5 = combined_df.values.tolist()\n",
    "train_loader_5 = torch.utils.data.DataLoader(train_data_5, batch_size=64, shuffle=True)\n",
    "#repeat for test data\n",
    "df1 = pd.DataFrame(sentence1_final_test_5, columns = [\"S1\", \"P1\", \"O1\"])\n",
    "df2 = pd.DataFrame(sentence2_final_test_5, columns = [\"S2\", \"P2\", \"O2\"])\n",
    "df3 = pd.DataFrame(labels_test_5, columns = [\"label\"])\n",
    "combined_df = df1.join(df2)\n",
    "combined_df = combined_df.join(df3)\n",
    "test_data_5 = combined_df.values.tolist()\n",
    "val_loader_5 = torch.utils.data.DataLoader(test_data_5, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972cfd03",
   "metadata": {},
   "source": [
    "I couldn't tell from the original paper if they had trained the model on each dataset in isolation (i.e generating a new instance of the model for each dataset), or trained one model with all the datasets. I opted for the first approach. Note the only transformation I do on the Manhattan distance this time is scaling it by a factor of 100; the original paper notes that we don't need to do the same transform as in Experiment 1 as the score values are continuous instead of binary this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5245d729",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13 [00:00<?, ?it/s]C:\\Users\\rbrow\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:149: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  C:\\b\\abs_bao0hdcrdh\\croot\\pytorch_1675190257512\\work\\torch\\csrc\\utils\\tensor_numpy.cpp:178.)\n",
      "  return default_collate([torch.as_tensor(b) for b in batch])\n",
      "100%|██████████| 13/13 [00:00<00:00, 50.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: curr_epoch_loss=3.930372953414917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 52.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: curr_epoch_loss=3.9497084617614746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 51.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: curr_epoch_loss=3.955994129180908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 53.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: curr_epoch_loss=3.8489224910736084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 50.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: curr_epoch_loss=3.942751407623291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 51.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: curr_epoch_loss=3.8198726177215576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 54.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: curr_epoch_loss=4.14233922958374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 53.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: curr_epoch_loss=3.9616148471832275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 51.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: curr_epoch_loss=3.8235697746276855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 52.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: curr_epoch_loss=3.887640953063965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 54.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: curr_epoch_loss=4.999321460723877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 56.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: curr_epoch_loss=5.482011795043945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 56.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: curr_epoch_loss=4.972720146179199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 56.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: curr_epoch_loss=5.366934299468994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 57.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: curr_epoch_loss=5.792056083679199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 53.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: curr_epoch_loss=5.736116409301758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 52.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: curr_epoch_loss=5.5301103591918945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 53.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: curr_epoch_loss=5.271693229675293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 57.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: curr_epoch_loss=5.0240888595581055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 54.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: curr_epoch_loss=5.235907554626465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 49.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: curr_epoch_loss=3.214266777038574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 50.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: curr_epoch_loss=3.0992441177368164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 52.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: curr_epoch_loss=3.105893611907959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 49.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: curr_epoch_loss=3.155287027359009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 50.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: curr_epoch_loss=2.953911781311035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 51.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: curr_epoch_loss=2.986299991607666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 50.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: curr_epoch_loss=3.0852112770080566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 51.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: curr_epoch_loss=3.2000341415405273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 51.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: curr_epoch_loss=2.9050850868225098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 52.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: curr_epoch_loss=3.403782367706299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 52.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: curr_epoch_loss=6.493733882904053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 51.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: curr_epoch_loss=6.498775005340576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 52.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: curr_epoch_loss=6.496604919433594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 53.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: curr_epoch_loss=6.262085437774658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 51.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: curr_epoch_loss=6.288381099700928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 51.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: curr_epoch_loss=6.417821407318115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 49.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: curr_epoch_loss=6.320668697357178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 48.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: curr_epoch_loss=6.413120269775391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 52.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: curr_epoch_loss=6.185629367828369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 55.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: curr_epoch_loss=6.422279357910156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#now train the model\n",
    "criterion = nn.MSELoss()\n",
    "model = SimpleCNN()\n",
    "model_h = SimpleCNN()\n",
    "model_4 = SimpleCNN()\n",
    "model_5 = SimpleCNN()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "n_epochs = 10\n",
    "from scipy.spatial.distance import cityblock\n",
    "import math\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "def train_model(model, train_dataloader, n_epoch=n_epochs, optimizer=optimizer, criterion=criterion):\n",
    "    import torch.optim as optim\n",
    "    model.train() # prep model for training\n",
    "    for epoch in range(n_epoch):\n",
    "        curr_epoch_loss = []\n",
    "        for s1s, s1p, s1o, s2s, s2p, s2o, target in tqdm(train_dataloader):\n",
    "            #first, process s1 and s2 through the model\n",
    "            #ensure the batch size is accurate\n",
    "            batch = s1s.shape[0]\n",
    "            s1 = np.concatenate([s1s,s1p,s1o])\n",
    "            s1 = np.reshape(s1, (batch,50,3,1))\n",
    "            s1 = torch.tensor(s1) \n",
    "            s1_processed = model(s1)\n",
    "            s2 = np.concatenate([s2s,s2p,s2o])\n",
    "            s2 = np.reshape(s2,(batch,50,3,1))\n",
    "            s2 = torch.tensor(s2)\n",
    "            s2_processed = model(s2)\n",
    "            #need to detach to perform manhattan distance calculation, otherwise throws error\n",
    "            s1_detached = s1_processed.detach()\n",
    "            s2_detached = s2_processed.detach()\n",
    "            y_hats = torch.empty(target.shape[0])\n",
    "            for i in range(target.shape[0]):\n",
    "                s1_detached_i = torch.flatten(s1_detached)\n",
    "                s2_detached_i = torch.flatten(s2_detached)\n",
    "                #now calculate manhattan distance\n",
    "                manhattan = cityblock(s1_detached_i, s2_detached_i)\n",
    "                y_hat = torch.tensor(manhattan) * 100 #based on observed output values from model, need to multiply to scale values\n",
    "                y_hats[i] = y_hat\n",
    "            y_hats = y_hats.requires_grad_()\n",
    "            target = target.float()\n",
    "            loss = criterion(y_hats,target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            curr_epoch_loss.append(loss.cpu().data.numpy())\n",
    "        print(f\"Epoch {epoch}: curr_epoch_loss={np.mean(curr_epoch_loss)}\")\n",
    "    return model\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "model = train_model(model, train_loader)\n",
    "model_h = train_model(model_h, train_loader_h)\n",
    "model_4 = train_model(model_4, train_loader_4)\n",
    "model_5 = train_model(model_5, train_loader_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485edf35",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "The Pearson's correlation between the predicted and true values for each dataset is shown in the table below this cell. In the next cell, the cumulative runtime and memory usage is displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5857dfda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.32342672 0.32342672 0.32342672 0.32342672 0.32342672 0.32342672\n",
      " 0.32342672 0.32342672 0.32342672 0.32342672 0.32342672 0.32342672\n",
      " 0.32342672 0.32342672 0.32342672 0.32342672 0.32342672 0.32342672\n",
      " 0.32342672 0.32342672 0.32342672 0.32342672 0.32342672 0.32342672\n",
      " 0.32342672]\n",
      "                   0\n",
      "MSRvid      0.176221\n",
      "headlines        NaN\n",
      "images2014  0.065386\n",
      "images2015 -0.030861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rbrow\\anaconda3\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model on the test data\n",
    "def eval_model(model, dataloader):\n",
    "    model.eval()\n",
    "    Y_pred = []\n",
    "    Y_true = []\n",
    "    for s1s, s1p, s1o, s2s, s2p, s2o, target in dataloader:\n",
    "        \n",
    "        batch = s1s.shape[0]\n",
    "        s1 = np.concatenate([s1s,s1p,s1o])\n",
    "        s1 = np.reshape(s1, (batch,50,3,1))\n",
    "        s1 = torch.tensor(s1)\n",
    "        s1_processed = model(s1)\n",
    "        s2 = np.concatenate([s2s,s2p,s2o])\n",
    "        s2 = np.reshape(s2,(batch,50,3,1))\n",
    "        s2 = torch.tensor(s2)\n",
    "        s2_processed = model(s2)\n",
    "        s1_detached = s1_processed.detach()\n",
    "        s2_detached = s2_processed.detach()\n",
    "        y_hats = torch.empty(target.shape[0])\n",
    "        for i in range(target.shape[0]):\n",
    "            s1_detached_i = torch.flatten(s1_detached)\n",
    "            s2_detached_i = torch.flatten(s2_detached)\n",
    "            #now calculate manhattan distance\n",
    "            manhattan = cityblock(s1_detached_i, s2_detached_i)\n",
    "            y_hat = torch.tensor(manhattan) * 100\n",
    "            y_hats[i] = y_hat\n",
    "        Y_pred.append(y_hats)\n",
    "        Y_true.append(target)\n",
    "    Y_pred = np.concatenate(Y_pred, axis=0)\n",
    "    Y_true = np.concatenate(Y_true, axis=0)\n",
    "    return Y_pred, Y_true\n",
    "\n",
    "#print metrics\n",
    "y_pred, y_true = eval_model(model, val_loader)\n",
    "y_pred_h, y_true_h = eval_model(model_h, val_loader_h)\n",
    "y_pred_4, y_true_4 = eval_model(model_4, val_loader_4)\n",
    "y_pred_5, y_true_5 = eval_model(model_5, val_loader_5)\n",
    "print(y_pred_h) #printed so you can see constant values\n",
    "#for Experiment 2, we want to find Pearson's correlation between truth and predicted\n",
    "from scipy import stats\n",
    "pMSRvid = stats.pearsonr(y_true, y_pred).statistic\n",
    "pheadlines = stats.pearsonr(y_true_h, y_pred_h).statistic\n",
    "pimages2014 = stats.pearsonr(y_true_4, y_pred_4).statistic\n",
    "pimages2015 = stats.pearsonr(y_true_5, y_pred_5).statistic\n",
    "headers = [\"MSRvid\", \"headlines\", \"images2014\", \"images2015\"]\n",
    "stats = [pMSRvid, pheadlines, pimages2014, pimages2015]\n",
    "print(pd.DataFrame(stats,headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cbdcf62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total running time = 152.84 seconds\n",
      "Current and Peak Memory Usage:\n",
      "(59204048, 59572440)\n"
     ]
    }
   ],
   "source": [
    "print(\"Total running time = {:.2f} seconds\".format(time.time() - _START_RUNTIME))\n",
    "print(\"Current and Peak Memory Usage:\")\n",
    "print(tracemalloc.get_traced_memory())\n",
    "tracemalloc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
