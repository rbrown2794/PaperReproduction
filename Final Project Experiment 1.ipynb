{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d817de0",
   "metadata": {},
   "source": [
    "# CS 598 DLH Final Project Experiment 1\n",
    "by Regan Brown (rnbrown3, group 51)\n",
    "\n",
    "This is NOT intended as a bonus \"Descriptive notebook\". This contains all the source code for Experiment 1 of my final project.\n",
    "\n",
    "The paper I attempted to reproduce is paper 151, \"Disease Prediction and Early Intervention System Based on Symptom Similarity Analysis\" by Peiying Zhang, Xingzhe Huang, and Maozhen Li. You may access the paper here: https://ieeexplore.ieee.org/document/8924757\n",
    "\n",
    "There is no code repo for this paper that I could find.\n",
    "\n",
    "In order to ensure the proper environment to run this code, please download the latest versions of all libraries/imports mentioned in the code blocks. In addition, you will need to install necessary CoreNLP packages and start up an instance of the Stanford Core NLP server, following the instructions at https://github.com/nltk/nltk/wiki/Stanford-CoreNLP-API-in-NLTK\n",
    "\n",
    "You do not need to download the Microsoft Research Paraphrase corpus used for this experiment separately; the load_dataset library takes care of downloading the data for you.\n",
    "\n",
    "## Preprocessing\n",
    "The first steps to take are loading in the MSRP dataset, extracting the sentence pairs and scores/labels for each, and initializing the Stanford parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b88a3632",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (C:/Users/rbrow/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Found cached dataset glue (C:/Users/rbrow/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "#be sure to start up Stanford Parser server following steps here: https://github.com/nltk/nltk/wiki/Stanford-CoreNLP-API-in-NLTK\n",
    "from nltk.parse import CoreNLPParser\n",
    "from nltk.tree import Tree, ParentedTree\n",
    "from datasets import load_dataset\n",
    "import time #for tracking time to run\n",
    "import tracemalloc #for tracking memory usage\n",
    "_START_RUNTIME = time.time()\n",
    "tracemalloc.start()\n",
    "\n",
    "#first, load in the MSRP training data\n",
    "df = load_dataset('glue', 'mrpc', split='train')\n",
    "labels = df['label']\n",
    "sentence1 = df['sentence1']\n",
    "sentence2 = df['sentence2']\n",
    "\n",
    "#also load in the MSRP test data, we will preprocess this as well\n",
    "df = load_dataset('glue', 'mrpc', split='test')\n",
    "labels_test = df['label']\n",
    "sentence1_test = df['sentence1']\n",
    "sentence2_test = df['sentence2']\n",
    "\n",
    "#initialize the Stanford parser\n",
    "parser = CoreNLPParser(url='http://localhost:9000')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a38bea",
   "metadata": {},
   "source": [
    "Next, I define the subject-predicate-object (SPO) algorithm we will use to parse the sentences with the Stanford parser, then identify the most relevant words in the sentence that provide the most meaning (i.e. the sentence trunk). My implementation mostly follows the pseudocode in the original paper, but I found their pseudocode for identifying the object did not return correct results. Instead, I based my implementation for finding the object on code from Github user Hassan Elmadany, found here: https://github.com/HassanElmadany/Extract-SVO/blob/master/Subject_Verb_Object_Extractor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cd77a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementation of SPO algorithm as outlined in the paper's pseudocode (Algorithm 1)\n",
    "#to help make sense of this code, please check the label definitions here: https://stackoverflow.com/questions/1833252/java-stanford-nlp-part-of-speech-labels\n",
    "def spo(sentence):\n",
    "    tree = parser.raw_parse(sentence)\n",
    "    tree = next(tree) #need to pull the Tree out of the iter\n",
    "    \n",
    "    subject = \"\"\n",
    "    predicate = \"\"\n",
    "    obj = \"\"\n",
    "    for t in tree[0]:\n",
    "        if t.label() == 'NP': #identify subject\n",
    "            for s in t.subtrees():\n",
    "                for n in s.subtrees():\n",
    "                    if n.label().startswith(\"NN\"):\n",
    "                        subject = n[0]\n",
    "        if t.label() == 'VP': #identify predicate\n",
    "            for p in t.subtrees():\n",
    "                for m in p.subtrees():\n",
    "                    if m.label().startswith(\"VB\"):\n",
    "                        predicate = m[0]\n",
    "        if t.label() == 'VP': #identify object (code based on code found here: https://github.com/HassanElmadany/Extract-SVO/blob/master/Subject_Verb_Object_Extractor.py)\n",
    "            for k in t.subtrees(lambda n: n.label() in ['NP', 'PP', 'ADJP']):\n",
    "                if k.label() in ['NP', 'PP']:\n",
    "                    for c in k.subtrees(lambda c: c.label().startswith('NN')):\n",
    "                        obj = c[0]\n",
    "                else:\n",
    "                    for c in k.subtrees(lambda c: c.label().startswith('JJ')):\n",
    "                        obj = c[0]\n",
    "    return [subject, predicate, obj]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77f67ab",
   "metadata": {},
   "source": [
    "Now that the SPO algorithm is defined, I call it on all the sentences in the sentence pairs and store the results. This process takes up nearly all the runtime, so be prepared to wait around 15 minutes if you run this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea284b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse first sentences in sentence pairs, for both train and test sets\n",
    "sentence1_parsed = []\n",
    "for s in sentence1:\n",
    "    parsed = spo(s)\n",
    "    sentence1_parsed.append(parsed)\n",
    "sentence1_parsed_test = []\n",
    "for s in sentence1_test:\n",
    "    parsed = spo(s)\n",
    "    sentence1_parsed_test.append(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e19c2618",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse second sentences in sentence pairs, for both train and test sets\n",
    "sentence2_parsed = []\n",
    "for s in sentence2:\n",
    "    parsed = spo(s)\n",
    "    sentence2_parsed.append(parsed)\n",
    "sentence2_parsed_test = []\n",
    "for s in sentence2_test:\n",
    "    parsed = spo(s)\n",
    "    sentence2_parsed_test.append(parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fab08a",
   "metadata": {},
   "source": [
    "If we were to look at the results of the parsing we just did, we would see that some of the sentences did not have an identified subject, predicate and/or object. This means our algorithm did not capture the meaning of that sentence, so comparisions against it can introduce inaccuracy into our model. So below, I do some cleaning to remove any sentence pairs where at least one of the sentences has a missing subject, predicate, or object. Also, I print out some data frames containing the cleaned train and test data, respectively, so you can get a sense of what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbc76f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S1</th>\n",
       "      <th>P1</th>\n",
       "      <th>O1</th>\n",
       "      <th>S2</th>\n",
       "      <th>P2</th>\n",
       "      <th>O2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amrozi</td>\n",
       "      <td>distorting</td>\n",
       "      <td>evidence</td>\n",
       "      <td>Amrozi</td>\n",
       "      <td>distorting</td>\n",
       "      <td>evidence</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yucaipa</td>\n",
       "      <td>selling</td>\n",
       "      <td>Safeway</td>\n",
       "      <td>Yucaipa</td>\n",
       "      <td>sold</td>\n",
       "      <td>Safeway</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>shares</td>\n",
       "      <td>set</td>\n",
       "      <td>high</td>\n",
       "      <td>shares</td>\n",
       "      <td>closing</td>\n",
       "      <td>high</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stock</td>\n",
       "      <td>close</td>\n",
       "      <td>Exchange</td>\n",
       "      <td>shares</td>\n",
       "      <td>jumped</td>\n",
       "      <td>Friday</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>year</td>\n",
       "      <td>dropped</td>\n",
       "      <td>period</td>\n",
       "      <td>year</td>\n",
       "      <td>dropped</td>\n",
       "      <td>period</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3661</th>\n",
       "      <td>Department</td>\n",
       "      <td>contain</td>\n",
       "      <td>infection</td>\n",
       "      <td>spokesperson</td>\n",
       "      <td>following</td>\n",
       "      <td>protocol</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3662</th>\n",
       "      <td>rules</td>\n",
       "      <td>reach</td>\n",
       "      <td>percent</td>\n",
       "      <td>limit</td>\n",
       "      <td>reaching</td>\n",
       "      <td>percent</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3664</th>\n",
       "      <td>Martin</td>\n",
       "      <td>serving</td>\n",
       "      <td>Barras</td>\n",
       "      <td>Martin</td>\n",
       "      <td>wounding</td>\n",
       "      <td>Fearon</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3666</th>\n",
       "      <td>notification</td>\n",
       "      <td>reported</td>\n",
       "      <td>MSNBC</td>\n",
       "      <td>MSNBC.com</td>\n",
       "      <td>reported</td>\n",
       "      <td>Friday</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3667</th>\n",
       "      <td>RR</td>\n",
       "      <td>rose</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>RR</td>\n",
       "      <td>grew</td>\n",
       "      <td>percent</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2053 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                S1          P1         O1            S2          P2        O2  \\\n",
       "0           Amrozi  distorting   evidence        Amrozi  distorting  evidence   \n",
       "1          Yucaipa     selling    Safeway       Yucaipa        sold   Safeway   \n",
       "3           shares         set       high        shares     closing      high   \n",
       "4            stock       close   Exchange        shares      jumped    Friday   \n",
       "5             year     dropped     period          year     dropped    period   \n",
       "...            ...         ...        ...           ...         ...       ...   \n",
       "3661    Department     contain  infection  spokesperson   following  protocol   \n",
       "3662         rules       reach    percent         limit    reaching   percent   \n",
       "3664        Martin     serving     Barras        Martin    wounding    Fearon   \n",
       "3666  notification    reported      MSNBC     MSNBC.com    reported    Friday   \n",
       "3667            RR        rose  Wednesday            RR        grew   percent   \n",
       "\n",
       "      label  \n",
       "0         1  \n",
       "1         0  \n",
       "3         0  \n",
       "4         1  \n",
       "5         1  \n",
       "...     ...  \n",
       "3661      1  \n",
       "3662      1  \n",
       "3664      0  \n",
       "3666      1  \n",
       "3667      0  \n",
       "\n",
       "[2053 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S1</th>\n",
       "      <th>P1</th>\n",
       "      <th>O1</th>\n",
       "      <th>S2</th>\n",
       "      <th>P2</th>\n",
       "      <th>O2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sales</td>\n",
       "      <td>expected</td>\n",
       "      <td>backlash</td>\n",
       "      <td>Co.</td>\n",
       "      <td>prompted</td>\n",
       "      <td>backlash</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>storm</td>\n",
       "      <td>hit</td>\n",
       "      <td>Monday</td>\n",
       "      <td>storm</td>\n",
       "      <td>hits</td>\n",
       "      <td>coast</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Quaife</td>\n",
       "      <td>remained</td>\n",
       "      <td>operation</td>\n",
       "      <td>Quaife</td>\n",
       "      <td>was</td>\n",
       "      <td>unprecedented</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>aide</td>\n",
       "      <td>allied</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>aide</td>\n",
       "      <td>allied</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SPX</td>\n",
       "      <td>was</td>\n",
       "      <td>percent</td>\n",
       "      <td>IXIC</td>\n",
       "      <td>was</td>\n",
       "      <td>percent</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1715</th>\n",
       "      <td>Yankees</td>\n",
       "      <td>took</td>\n",
       "      <td>pick</td>\n",
       "      <td>Yankees</td>\n",
       "      <td>selected</td>\n",
       "      <td>pick</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1718</th>\n",
       "      <td>Crews</td>\n",
       "      <td>dump</td>\n",
       "      <td>rain</td>\n",
       "      <td>Crews</td>\n",
       "      <td>use</td>\n",
       "      <td>travel</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1719</th>\n",
       "      <td>directors</td>\n",
       "      <td>completed</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>acquisition</td>\n",
       "      <td>close</td>\n",
       "      <td>quarter</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1722</th>\n",
       "      <td>Hamilton</td>\n",
       "      <td>remained</td>\n",
       "      <td>attack</td>\n",
       "      <td>morning</td>\n",
       "      <td>talked</td>\n",
       "      <td>attack</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723</th>\n",
       "      <td>Corp</td>\n",
       "      <td>accept</td>\n",
       "      <td>offer</td>\n",
       "      <td>news</td>\n",
       "      <td>accept</td>\n",
       "      <td>offer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>957 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             S1         P1         O1           S2        P2             O2  \\\n",
       "1         sales   expected   backlash          Co.  prompted       backlash   \n",
       "3         storm        hit     Monday        storm      hits          coast   \n",
       "6        Quaife   remained  operation       Quaife       was  unprecedented   \n",
       "8          aide     allied   Thursday         aide    allied       Thursday   \n",
       "9           SPX        was    percent         IXIC       was        percent   \n",
       "...         ...        ...        ...          ...       ...            ...   \n",
       "1715    Yankees       took       pick      Yankees  selected           pick   \n",
       "1718      Crews       dump       rain        Crews       use         travel   \n",
       "1719  directors  completed     Nvidia  acquisition     close        quarter   \n",
       "1722   Hamilton   remained     attack      morning    talked         attack   \n",
       "1723       Corp     accept      offer         news    accept          offer   \n",
       "\n",
       "      label  \n",
       "1         1  \n",
       "3         0  \n",
       "6         0  \n",
       "8         1  \n",
       "9         0  \n",
       "...     ...  \n",
       "1715      1  \n",
       "1718      0  \n",
       "1719      1  \n",
       "1722      0  \n",
       "1723      1  \n",
       "\n",
       "[957 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#sentences where SPO could not parse out any of the subject, predicate, or object are meaningless to us\n",
    "#Since accurate comparisons cannot be made, remove any pairs affected by this\n",
    "import pandas as pd\n",
    "df1 = pd.DataFrame(sentence1_parsed, columns = [\"S1\", \"P1\", \"O1\"])\n",
    "df2 = pd.DataFrame(sentence2_parsed, columns = [\"S2\", \"P2\", \"O2\"])\n",
    "df3 = pd.DataFrame(labels, columns = [\"label\"])\n",
    "combined_df = df1.join(df2)\n",
    "combined_df = combined_df.join(df3)\n",
    "cleaned_df = combined_df[(combined_df.S1 != '') & (combined_df.P1 != '') & (combined_df.O1 != '')]\n",
    "cleaned_df = cleaned_df[(cleaned_df.S2 != '') & (cleaned_df.P2 != '') & (cleaned_df.O2 != '')]\n",
    "display(cleaned_df)\n",
    "#now split these back out into separate lists; still need to process those via Word2Vec\n",
    "df1 = cleaned_df.iloc[:,:3]\n",
    "df2 = cleaned_df.iloc[:,3:6]\n",
    "df3 = cleaned_df.iloc[:,6:]\n",
    "sentence1_cleaned = df1.values.tolist()\n",
    "sentence2_cleaned = df2.values.tolist()\n",
    "labels = df3.values.tolist()\n",
    "\n",
    "#now do the same thing for test data\n",
    "df1 = pd.DataFrame(sentence1_parsed_test, columns = [\"S1\", \"P1\", \"O1\"])\n",
    "df2 = pd.DataFrame(sentence2_parsed_test, columns = [\"S2\", \"P2\", \"O2\"])\n",
    "df3 = pd.DataFrame(labels_test, columns = [\"label\"])\n",
    "combined_df = df1.join(df2)\n",
    "combined_df = combined_df.join(df3)\n",
    "cleaned_df = combined_df[(combined_df.S1 != '') & (combined_df.P1 != '') & (combined_df.O1 != '')]\n",
    "cleaned_df = cleaned_df[(cleaned_df.S2 != '') & (cleaned_df.P2 != '') & (cleaned_df.O2 != '')]\n",
    "display(cleaned_df)\n",
    "#now split these back out into separate lists; still need to process those via Word2Vec\n",
    "df1 = cleaned_df.iloc[:,:3]\n",
    "df2 = cleaned_df.iloc[:,3:6]\n",
    "df3 = cleaned_df.iloc[:,6:]\n",
    "sentence1_cleaned_test = df1.values.tolist()\n",
    "sentence2_cleaned_test = df2.values.tolist()\n",
    "labels_test = df3.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accb9c24",
   "metadata": {},
   "source": [
    "We are almost done with preprocessing! Next, we need to use Word2Vec to convert the words in the pre-processed sentences into numerical vectors. Since our dataset isn't very large (in the kilobytes), we end up with small Word2Vec models. We also make sure to transpose the vectors, as that is what the paper specifies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f4c98d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2Vec conversion. Dimensions and procedure match what's in the paper\n",
    "#Point of ambiguity: we only ever have a single word for a subject, predicate, or object; but paper seems to suggest sometimes\n",
    "#that there can be multi-word subjects/predicates/objects\n",
    "import os\n",
    "import numpy as np\n",
    "RANDOM_SEED = 23432098\n",
    "os.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "#build a Word2Vec model for train data and one for test data\n",
    "train_sentences = sentence1_cleaned + sentence2_cleaned\n",
    "test_sentences = sentence1_cleaned_test + sentence2_cleaned_test\n",
    "w2v1 = Word2Vec(train_sentences, vector_size=50, workers=1, min_count=1)\n",
    "w2v1_test = Word2Vec(test_sentences, vector_size=50, workers=1, min_count=1)\n",
    "\n",
    "#then to get the sentences_final, pull out the .wv for each word in the sentence and transpose it\n",
    "sentence1_final = []\n",
    "sentence2_final = []\n",
    "sentence1_final_test = []\n",
    "sentence2_final_test = []\n",
    "for s in sentence1_cleaned:\n",
    "    words = []\n",
    "    for w in s:\n",
    "        mat = w2v1.wv[w]\n",
    "        words.append(mat.transpose())\n",
    "    sentence1_final.append(words)\n",
    "for s in sentence2_cleaned:\n",
    "    words = []\n",
    "    for w in s:\n",
    "        mat = w2v1.wv[w]\n",
    "        words.append(mat.transpose())\n",
    "    sentence2_final.append(words)\n",
    "for s in sentence1_cleaned_test:\n",
    "    words = []\n",
    "    for w in s:\n",
    "        mat = w2v1_test.wv[w]\n",
    "        words.append(mat.transpose())\n",
    "    sentence1_final_test.append(words)\n",
    "for s in sentence2_cleaned_test:\n",
    "    words = []\n",
    "    for w in s:\n",
    "        mat = w2v1_test.wv[w]\n",
    "        words.append(mat.transpose())\n",
    "    sentence2_final_test.append(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b17beb",
   "metadata": {},
   "source": [
    "## Model Definition and Training\n",
    "I have opted not to include a pretrained copy of the model because 1) its actual training time is negligible, and you can parse the data to train it with in a matter of minutes and 2) it does not produce helpful results anyway. Below, you can find my model definition. I tried to match its construction with the details in the paper, but there was some missing/ambiguous info on hidden nodes, etc that I had to guess on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc05ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the CNN model\n",
    "#I am using MaxPool2d as opposed to k-max pooling as we know the sentences should always be the same size\n",
    "#I have left the print statements intact so you can see how the values become smaller/closer to zero\n",
    "#just uncomment, then run to see\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(50, 17, kernel_size=(3,3), padding=1)\n",
    "        self.conv2 = nn.Conv2d(17, 6, kernel_size=(3,3), padding=1)\n",
    "        #self.pool1 = nn.MaxPool2d(3) #errors if set to 3 as the original paper uses for its pooling\n",
    "        self.pool2 = nn.MaxPool2d(1)\n",
    "        self.fc1 = nn.Linear(18, 1)\n",
    "\n",
    "    def forward(self, x_prime):\n",
    "        x_prime = F.relu(self.conv1(x_prime))\n",
    "        #print(\"After first conv layer:\")\n",
    "        #print(x_prime)\n",
    "        x_prime = F.relu(self.conv2(x_prime))\n",
    "        #print(\"After second conv layer:\")\n",
    "        #print(x_prime)\n",
    "        x_prime = self.pool2(x_prime)\n",
    "        #print(\"After first pool:\")\n",
    "        #print(x_prime)\n",
    "        x_prime = self.pool2(x_prime)\n",
    "        #print(\"After second pool:\")\n",
    "        #print(x_prime)\n",
    "        x_prime = x_prime.view(-1, 18)\n",
    "        #print(\"View X prime:\")\n",
    "        #print(x_prime)\n",
    "        x_prime = self.fc1(x_prime)\n",
    "        #print(\"After FC layer:\")\n",
    "        #print(x_prime) #show what is being output from the model\n",
    "        return x_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061f7642",
   "metadata": {},
   "source": [
    "Now, we just need to do a little bit more manipulation of our training and test data to create our data loaders. Batch size is 64, following the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15b24e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the data loaders\n",
    "#to do this, construct the training data by binding together final sentence1 and sentence2 with their target score\n",
    "df1 = pd.DataFrame(sentence1_final, columns = [\"S1\", \"P1\", \"O1\"])\n",
    "df2 = pd.DataFrame(sentence2_final, columns = [\"S2\", \"P2\", \"O2\"])\n",
    "df3 = pd.DataFrame(labels, columns = [\"label\"])\n",
    "combined_df = df1.join(df2)\n",
    "combined_df = combined_df.join(df3)\n",
    "train_data = combined_df.values.tolist()\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "#repeat for test data\n",
    "df1 = pd.DataFrame(sentence1_final_test, columns = [\"S1\", \"P1\", \"O1\"])\n",
    "df2 = pd.DataFrame(sentence2_final_test, columns = [\"S2\", \"P2\", \"O2\"])\n",
    "df3 = pd.DataFrame(labels_test, columns = [\"label\"])\n",
    "combined_df = df1.join(df2)\n",
    "combined_df = combined_df.join(df3)\n",
    "test_data = combined_df.values.tolist()\n",
    "val_loader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13044582",
   "metadata": {},
   "source": [
    "Below is where we actually train the model. Each sentence in the sentence pair is passed through the CNN model separately; then I follow the methodology in the paper to determine their similarity score using Manhattan distance and using it as an exponent of e to get a score between 0 (dissimilar) and 1 (similar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fea36497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/33 [00:00<?, ?it/s]C:\\Users\\rbrow\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:149: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  C:\\b\\abs_bao0hdcrdh\\croot\\pytorch_1675190257512\\work\\torch\\csrc\\utils\\tensor_numpy.cpp:178.)\n",
      "  return default_collate([torch.as_tensor(b) for b in batch])\n",
      "100%|██████████| 33/33 [00:00<00:00, 48.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: curr_epoch_loss=0.3270833194255829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:00<00:00, 54.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: curr_epoch_loss=0.3326704502105713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:00<00:00, 54.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: curr_epoch_loss=0.32149621844291687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:00<00:00, 54.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: curr_epoch_loss=0.3438447117805481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:00<00:00, 52.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: curr_epoch_loss=0.3326704502105713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:00<00:00, 54.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: curr_epoch_loss=0.3438447117805481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:00<00:00, 54.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: curr_epoch_loss=0.3382575809955597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:00<00:00, 54.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: curr_epoch_loss=0.32149621844291687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:00<00:00, 54.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: curr_epoch_loss=0.3326704502105713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:00<00:00, 53.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: curr_epoch_loss=0.3326704502105713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#now train the model\n",
    "criterion = nn.MSELoss()\n",
    "model = SimpleCNN()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "n_epochs = 10\n",
    "from scipy.spatial.distance import cityblock\n",
    "import math\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "def train_model(model, train_dataloader, n_epoch=n_epochs, optimizer=optimizer, criterion=criterion):\n",
    "    import torch.optim as optim\n",
    "    model.train() # prep model for training\n",
    "    for epoch in range(n_epoch):\n",
    "        curr_epoch_loss = []\n",
    "        for s1s, s1p, s1o, s2s, s2p, s2o, target in tqdm(train_dataloader):\n",
    "            #first, process s1 and s2 through the model\n",
    "            #ensure the batch size is accurate\n",
    "            batch = s1s.shape[0]\n",
    "            s1 = np.concatenate([s1s,s1p,s1o])\n",
    "            s1 = np.reshape(s1, (batch,50,3,1))\n",
    "            s1 = torch.tensor(s1) \n",
    "            s1_processed = model(s1)\n",
    "            s2 = np.concatenate([s2s,s2p,s2o])\n",
    "            s2 = np.reshape(s2,(batch,50,3,1))\n",
    "            s2 = torch.tensor(s2)\n",
    "            s2_processed = model(s2)\n",
    "            #need to detach to perform manhattan distance calculation, otherwise throws error\n",
    "            s1_detached = s1_processed.detach()\n",
    "            s2_detached = s2_processed.detach()\n",
    "            y_hats = torch.empty(target.shape[0])\n",
    "            for i in range(target.shape[0]):\n",
    "                s1_detached_i = torch.flatten(s1_detached)\n",
    "                s2_detached_i = torch.flatten(s2_detached)\n",
    "                #now calculate manhattan distance\n",
    "                manhattan = cityblock(s1_detached_i, s2_detached_i)\n",
    "                y_hat = math.e ** (-manhattan)\n",
    "                #normalize y_hat score to 0 or 1 for MSRP data\n",
    "                if y_hat >= 0.5:\n",
    "                    y_hat = 1\n",
    "                else:\n",
    "                    y_hat = 0\n",
    "                y_hats[i] = y_hat\n",
    "            y_hats = y_hats.requires_grad_()\n",
    "            target = target.float()\n",
    "            loss = criterion(y_hats,target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            curr_epoch_loss.append(loss.cpu().data.numpy())\n",
    "        print(f\"Epoch {epoch}: curr_epoch_loss={np.mean(curr_epoch_loss)}\")\n",
    "    return model\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "model = train_model(model, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa5da2d",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "The evalution code is below. For Experiment 1, the authors only took note of their attained accuracy and F score. I also included precision and recall in my results table for fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d32a3759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  0\n",
      "Accuracy   0.668757\n",
      "F Score    0.801503\n",
      "Precision  0.668757\n",
      "Recall     1.000000\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model on the test data\n",
    "def eval_model(model, dataloader):\n",
    "    model.eval()\n",
    "    Y_pred = []\n",
    "    Y_true = []\n",
    "    for s1s, s1p, s1o, s2s, s2p, s2o, target in dataloader:\n",
    "        \n",
    "        batch = s1s.shape[0]\n",
    "        s1 = np.concatenate([s1s,s1p,s1o])\n",
    "        s1 = np.reshape(s1, (batch,50,3,1))\n",
    "        s1 = torch.tensor(s1)\n",
    "        s1_processed = model(s1)\n",
    "        s2 = np.concatenate([s2s,s2p,s2o])\n",
    "        s2 = np.reshape(s2,(batch,50,3,1))\n",
    "        s2 = torch.tensor(s2)\n",
    "        s2_processed = model(s2)\n",
    "        s1_detached = s1_processed.detach()\n",
    "        s2_detached = s2_processed.detach()\n",
    "        y_hats = torch.empty(target.shape[0])\n",
    "        for i in range(target.shape[0]):\n",
    "            s1_detached_i = torch.flatten(s1_detached)\n",
    "            s2_detached_i = torch.flatten(s2_detached)\n",
    "            #now calculate manhattan distance\n",
    "            manhattan = cityblock(s1_detached_i, s2_detached_i)\n",
    "            y_hat = math.e ** (-manhattan)\n",
    "            #normalize y_hat score to 0 or 1 for MSRP data\n",
    "            if y_hat >= 0.5:\n",
    "                y_hat = 1\n",
    "            else:\n",
    "                y_hat = 0\n",
    "            y_hats[i] = y_hat\n",
    "        Y_pred.append(y_hats)\n",
    "        Y_true.append(target)\n",
    "    Y_pred = np.concatenate(Y_pred, axis=0)\n",
    "    Y_true = np.concatenate(Y_true, axis=0)\n",
    "    return Y_pred, Y_true\n",
    "#print metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "y_pred, y_true = eval_model(model, val_loader)\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "prec, recall, fscore, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "headers = [\"Accuracy\", \"F Score\", \"Precision\", \"Recall\"]\n",
    "stats = [acc, fscore, prec, recall]\n",
    "print(pd.DataFrame(stats,headers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b0c38f",
   "metadata": {},
   "source": [
    "Here is verification of the runtime and memory usage I listed in my report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec667833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total running time = 935.92 seconds\n",
      "Current and Peak Memory Usage:\n",
      "(35357838, 35483841)\n"
     ]
    }
   ],
   "source": [
    "print(\"Total running time = {:.2f} seconds\".format(time.time() - _START_RUNTIME))\n",
    "print(\"Current and Peak Memory Usage:\")\n",
    "print(tracemalloc.get_traced_memory())\n",
    "tracemalloc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
