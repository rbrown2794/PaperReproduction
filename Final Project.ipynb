{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b88a3632",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (C:/Users/rbrow/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Found cached dataset glue (C:/Users/rbrow/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "#be sure to start up Stanford Parser server following steps here: https://github.com/nltk/nltk/wiki/Stanford-CoreNLP-API-in-NLTK\n",
    "from nltk.parse import CoreNLPParser\n",
    "from nltk.tree import Tree\n",
    "from datasets import load_dataset\n",
    "import time #for tracking time to run\n",
    "import tracemalloc #for tracking memory usage\n",
    "_START_RUNTIME = time.time()\n",
    "tracemalloc.start()\n",
    "#first, load in the MSRP training data\n",
    "df = load_dataset('glue', 'mrpc', split='train')\n",
    "labels = df['label']\n",
    "sentence1 = df['sentence1']\n",
    "sentence2 = df['sentence2']\n",
    "#also load in the MSRP test data, we will preprocess this as well\n",
    "df = load_dataset('glue', 'mrpc', split='test')\n",
    "labels_test = df['label']\n",
    "sentence1_test = df['sentence1']\n",
    "sentence2_test = df['sentence2']\n",
    "#initialize the Standford parser\n",
    "parser = CoreNLPParser(url='http://localhost:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cd77a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementation of SPO algorithm as outlined in the paper's pseudocode (Algorithm 1)\n",
    "def spo(sentence):\n",
    "    tree = parser.raw_parse(sentence)\n",
    "    tree = next(tree) #need to pull the Tree out of the iter\n",
    "    \n",
    "    subject = \"\"\n",
    "    predicate = \"\"\n",
    "    obj = \"\"\n",
    "    for t in tree[0]:\n",
    "        if t.label() == 'NP':\n",
    "            for s in t.subtrees():\n",
    "                for n in s.subtrees():\n",
    "                    if n.label().startswith(\"NN\"):\n",
    "                        subject = n[0]\n",
    "        if t.label() == 'VP':\n",
    "            for p in t.subtrees():\n",
    "                for m in p.subtrees():\n",
    "                    if m.label().startswith(\"VB\"):\n",
    "                        predicate = m[0]\n",
    "        if t.label() == 'VP':\n",
    "            for k in t.subtrees():\n",
    "                for v in k.subtrees():\n",
    "                    if v.label() in (\"NP\",\"VP\"):\n",
    "                        for c in v.subtrees():\n",
    "                            if c.label().startswith(\"NN\"): #paper says to use JJ but that is adjective, use noun instead\n",
    "                                obj = c[0]\n",
    "                    else:\n",
    "                        for c in v.subtrees():\n",
    "                            if c.label().startswith(\"NN\"):\n",
    "                                obj = c[0]\n",
    "    return [subject, predicate, obj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea284b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse first sentence in sentence pair\n",
    "sentence1_parsed = []\n",
    "for s in sentence1:\n",
    "    parsed = spo(s)\n",
    "    sentence1_parsed.append(parsed)\n",
    "sentence1_parsed_test = []\n",
    "for s in sentence1_test:\n",
    "    parsed = spo(s)\n",
    "    sentence1_parsed_test.append(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e19c2618",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse second sentence in sentence pair\n",
    "sentence2_parsed = []\n",
    "for s in sentence2:\n",
    "    parsed = spo(s)\n",
    "    sentence2_parsed.append(parsed)\n",
    "sentence2_parsed_test = []\n",
    "for s in sentence2_test:\n",
    "    parsed = spo(s)\n",
    "    sentence2_parsed_test.append(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbc76f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentences where SPO could not parse out any of the subject, predicate, or object are meaningless to us\n",
    "#Since accurate comparisons cannot be made, remove any pairs affected by this\n",
    "import pandas as pd\n",
    "df1 = pd.DataFrame(sentence1_parsed, columns = [\"S1\", \"P1\", \"O1\"])\n",
    "df2 = pd.DataFrame(sentence2_parsed, columns = [\"S2\", \"P2\", \"O2\"])\n",
    "df3 = pd.DataFrame(labels, columns = [\"label\"])\n",
    "combined_df = df1.join(df2)\n",
    "combined_df = combined_df.join(df3)\n",
    "cleaned_df = combined_df[(combined_df.S1 != '') & (combined_df.P1 != '') & (combined_df.O1 != '')]\n",
    "cleaned_df = cleaned_df[(cleaned_df.S2 != '') & (cleaned_df.P2 != '') & (cleaned_df.O2 != '')]\n",
    "#now split these back out into separate lists; still need to process those via Word2Vec\n",
    "df1 = cleaned_df.iloc[:,:3]\n",
    "df2 = cleaned_df.iloc[:,3:6]\n",
    "df3 = cleaned_df.iloc[:,6:]\n",
    "sentence1_cleaned = df1.values.tolist()\n",
    "sentence2_cleaned = df2.values.tolist()\n",
    "labels = df3.values.tolist()\n",
    "#now do the same thing for test data\n",
    "df1 = pd.DataFrame(sentence1_parsed_test, columns = [\"S1\", \"P1\", \"O1\"])\n",
    "df2 = pd.DataFrame(sentence2_parsed_test, columns = [\"S2\", \"P2\", \"O2\"])\n",
    "df3 = pd.DataFrame(labels_test, columns = [\"label\"])\n",
    "combined_df = df1.join(df2)\n",
    "combined_df = combined_df.join(df3)\n",
    "cleaned_df = combined_df[(combined_df.S1 != '') & (combined_df.P1 != '') & (combined_df.O1 != '')]\n",
    "cleaned_df = cleaned_df[(cleaned_df.S2 != '') & (cleaned_df.P2 != '') & (cleaned_df.O2 != '')]\n",
    "#now split these back out into separate lists; still need to process those via Word2Vec\n",
    "df1 = cleaned_df.iloc[:,:3]\n",
    "df2 = cleaned_df.iloc[:,3:6]\n",
    "df3 = cleaned_df.iloc[:,6:]\n",
    "sentence1_cleaned_test = df1.values.tolist()\n",
    "sentence2_cleaned_test = df2.values.tolist()\n",
    "labels_test = df3.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f4c98d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2Vec conversion. The rationale for using it is to match dimensions and procedure with the paper.\n",
    "#In actuality, we only ever have a single word for a subject, predicate, or object.\n",
    "import os\n",
    "import numpy as np\n",
    "RANDOM_SEED = 23432098\n",
    "os.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "w2v1 = Word2Vec(sentence1_cleaned, vector_size=50, workers=1, min_count=1)\n",
    "w2v2 = Word2Vec(sentence2_cleaned, vector_size=50, workers=1, min_count=1)\n",
    "w2v1_test = Word2Vec(sentence1_cleaned_test, vector_size=50, workers=1, min_count=1)\n",
    "w2v2_test = Word2Vec(sentence2_cleaned_test, vector_size=50, workers=1, min_count=1)\n",
    "#so I'm not entirely sure what to do here. I think I will have 2 separate Word2Vec models.\n",
    "#then to get the sentences_final, pull out the .wv for each word in the sentence and transpose it\n",
    "sentence1_final = []\n",
    "sentence2_final = []\n",
    "sentence1_final_test = []\n",
    "sentence2_final_test = []\n",
    "for s in sentence1_cleaned:\n",
    "    words = []\n",
    "    for w in s:\n",
    "        mat = w2v1.wv[w]\n",
    "        words.append(mat.transpose())\n",
    "    sentence1_final.append(words)\n",
    "for s in sentence2_cleaned:\n",
    "    words = []\n",
    "    for w in s:\n",
    "        mat = w2v2.wv[w]\n",
    "        words.append(mat.transpose())\n",
    "    sentence2_final.append(words)\n",
    "for s in sentence1_cleaned_test:\n",
    "    words = []\n",
    "    for w in s:\n",
    "        mat = w2v1_test.wv[w]\n",
    "        words.append(mat.transpose())\n",
    "    sentence1_final_test.append(words)\n",
    "for s in sentence2_cleaned_test:\n",
    "    words = []\n",
    "    for w in s:\n",
    "        mat = w2v2_test.wv[w]\n",
    "        words.append(mat.transpose())\n",
    "    sentence2_final_test.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc05ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the CNN model\n",
    "#I am using MaxPool2d as opposed to k-max pooling as we know the sentences should always be the same size\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(50, 50, kernel_size=(3,3), padding=1)\n",
    "        self.conv2 = nn.Conv2d(50, 1, kernel_size=(3,3), padding=1)\n",
    "        self.pool = nn.MaxPool2d(1) #errors if set to 3\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.pool(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15b24e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the data loaders\n",
    "#to do this, construct the training data by binding together final sentence1 and sentence2 with their target score\n",
    "df1 = pd.DataFrame(sentence1_final, columns = [\"S1\", \"P1\", \"O1\"])\n",
    "df2 = pd.DataFrame(sentence2_final, columns = [\"S2\", \"P2\", \"O2\"])\n",
    "df3 = pd.DataFrame(labels, columns = [\"label\"])\n",
    "combined_df = df1.join(df2)\n",
    "combined_df = combined_df.join(df3)\n",
    "train_data = combined_df.values.tolist()\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "#repeat for test data\n",
    "df1 = pd.DataFrame(sentence1_final_test, columns = [\"S1\", \"P1\", \"O1\"])\n",
    "df2 = pd.DataFrame(sentence2_final_test, columns = [\"S2\", \"P2\", \"O2\"])\n",
    "df3 = pd.DataFrame(labels_test, columns = [\"label\"])\n",
    "combined_df = df1.join(df2)\n",
    "combined_df = combined_df.join(df3)\n",
    "test_data = combined_df.values.tolist()\n",
    "val_loader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fea36497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/33 [00:00<?, ?it/s]C:\\Users\\rbrow\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:149: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  C:\\b\\abs_bao0hdcrdh\\croot\\pytorch_1675190257512\\work\\torch\\csrc\\utils\\tensor_numpy.cpp:178.)\n",
      "  return default_collate([torch.as_tensor(b) for b in batch])\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 33/33 [00:00<00:00, 79.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: curr_epoch_loss=0.3316498398780823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#now prepare for training\n",
    "criterion = nn.MSELoss()\n",
    "model = SimpleCNN()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "n_epochs = 1\n",
    "from scipy.spatial.distance import cityblock\n",
    "import math\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "def train_model(model, train_dataloader, n_epoch=n_epochs, optimizer=optimizer, criterion=criterion):\n",
    "    import torch.optim as optim\n",
    "    model.train() # prep model for training\n",
    "    for epoch in range(n_epoch):\n",
    "        curr_epoch_loss = []\n",
    "        for s1s, s1p, s1o, s2s, s2p, s2o, target in tqdm(train_dataloader):\n",
    "            #first, process s1 and s2 through the model\n",
    "            #ensure the batch size is accurate\n",
    "            batch = s1s.shape[0]\n",
    "            s1 = np.concatenate([s1s,s1p,s1o])\n",
    "            s1 = np.reshape(s1, (batch,50,3,1))\n",
    "            s1 = torch.from_numpy(s1)\n",
    "            s1_processed = model(s1)\n",
    "            s2 = np.concatenate([s2s,s2p,s2o])\n",
    "            s2 = np.reshape(s2,(batch,50,3,1))\n",
    "            s2 = torch.from_numpy(s2)\n",
    "            s2_processed = model(s2)\n",
    "            #need to detach to perform manhattan distance calculation\n",
    "            s1_detached = s1_processed.detach()\n",
    "            s2_detached = s2_processed.detach()\n",
    "            y_hats = torch.empty(target.shape[0])\n",
    "            for i in range(target.shape[0]):\n",
    "                s1_detached_i = torch.squeeze(s1_detached[i,:,0,:])\n",
    "                s2_detached_i = torch.squeeze(s2_detached[i,:,0,:])\n",
    "                s1_detached_i = torch.unsqueeze(s1_detached_i, 0)\n",
    "                s2_detached_i = torch.unsqueeze(s2_detached_i, 0)\n",
    "                #now calculate manhattan distance\n",
    "                manhattan = cityblock(s1_detached_i, s2_detached_i)\n",
    "                y_hat = math.e ** (-manhattan)\n",
    "                #normalize y_hat score to 0 or 1 for MSRP data\n",
    "                if y_hat >= 0.5:\n",
    "                    y_hat = 1\n",
    "                else:\n",
    "                    y_hat = 0\n",
    "                y_hats[i] = y_hat\n",
    "            y_hats = y_hats.requires_grad_()\n",
    "            target = target.float()\n",
    "            loss = criterion(y_hats,target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            curr_epoch_loss.append(loss.cpu().data.numpy())\n",
    "        print(f\"Epoch {epoch}: curr_epoch_loss={np.mean(curr_epoch_loss)}\")\n",
    "    return model\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "model = train_model(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d32a3759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6704312114989733\n",
      "0.6704312114989733\n",
      "1.0\n",
      "0.8027043638598649\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model on the test data\n",
    "def eval_model(model, dataloader):\n",
    "    model.eval()\n",
    "    Y_pred = []\n",
    "    Y_true = []\n",
    "    for s1s, s1p, s1o, s2s, s2p, s2o, target in dataloader:\n",
    "        \n",
    "        batch = s1s.shape[0]\n",
    "        s1 = np.concatenate([s1s,s1p,s1o])\n",
    "        s1 = np.reshape(s1, (batch,50,3,1))\n",
    "        s1 = torch.from_numpy(s1)\n",
    "        s1_processed = model(s1)\n",
    "        s2 = np.concatenate([s2s,s2p,s2o])\n",
    "        s2 = np.reshape(s2,(batch,50,3,1))\n",
    "        s2 = torch.from_numpy(s2)\n",
    "        s2_processed = model(s2)\n",
    "        s1_detached = s1_processed.detach()\n",
    "        s2_detached = s2_processed.detach()\n",
    "        y_hats = torch.empty(target.shape[0])\n",
    "        for i in range(target.shape[0]):\n",
    "            s1_detached_i = torch.squeeze(s1_detached[i,:,0,:])\n",
    "            s2_detached_i = torch.squeeze(s2_detached[i,:,0,:])\n",
    "            s1_detached_i = torch.unsqueeze(s1_detached_i, 0)\n",
    "            s2_detached_i = torch.unsqueeze(s2_detached_i, 0)\n",
    "            #now calculate manhattan distance\n",
    "            manhattan = cityblock(s1_detached_i, s2_detached_i)\n",
    "            y_hat = math.e ** (-manhattan)\n",
    "            #normalize y_hat score to 0 or 1 for MSRP data\n",
    "            if y_hat >= 0.5:\n",
    "                y_hat = 1\n",
    "            else:\n",
    "                y_hat = 0\n",
    "            y_hats[i] = y_hat\n",
    "        Y_pred.append(y_hats)\n",
    "        Y_true.append(target)\n",
    "    Y_pred = np.concatenate(Y_pred, axis=0)\n",
    "    Y_true = np.concatenate(Y_true, axis=0)\n",
    "    return Y_pred, Y_true\n",
    "#print metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "y_pred, y_true = eval_model(model, val_loader)\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "prec, recall, fscore, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "print(acc)\n",
    "print(prec)\n",
    "print(recall)\n",
    "print(fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec667833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total running time = 779.28 seconds\n",
      "(36628415, 36962799)\n"
     ]
    }
   ],
   "source": [
    "print(\"Total running time = {:.2f} seconds\".format(time.time() - _START_RUNTIME))\n",
    "print(tracemalloc.get_traced_memory())\n",
    "tracemalloc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
